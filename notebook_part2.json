[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "---\n",
   "<a id=\"section-4\"></a>\n",
   "# Section 4: Deploy Prototype\n",
   "\n",
   "## Deployment Strategy\n",
   "\n",
   "Both prototypes are designed as **static web applications** that can be deployed with minimal infrastructure:\n",
   "\n",
   "### Deployment Options (Simplest to Most Complex)\n",
   "\n",
   "| Method | Complexity | Requirements | Best For |\n",
   "|:---|:---|:---|:---|\n",
   "| **Local file** | Trivial | Web browser only | Individual testing |\n",
   "| **Python HTTP server** | Very Low | Python installed | Lab sessions |\n",
   "| **GitHub Pages** | Low | GitHub account | Persistent hosting |\n",
   "| **Netlify/Vercel** | Low | Account signup | Production-like |\n",
   "\n",
   "### Telemetry Architecture\n",
   "\n",
   "Both prototypes include client-side telemetry that captures:\n",
   "1. **Navigation events** — Page views, timestamps, time-on-page\n",
   "2. **Interaction events** — Clicks, form submissions, choices\n",
   "3. **Feature usage** — Which features are accessed and how often\n",
   "4. **Session metadata** — Duration, browser info, errors\n",
   "\n",
   "Data is stored in the browser's memory (`window.telemetryLog` array) and exported as JSON files per participant. This approach:\n",
   "- Requires **no server-side infrastructure** for logging\n",
   "- Gives participants **full transparency** over collected data\n",
   "- Enables **easy integration** with analysis pipelines\n",
   "- Follows **data minimization** principles (GDPR-aligned)\n",
   "\n",
   "### Methodological Consideration\n",
   "For a CHI paper, using client-side telemetry is acceptable for prototype evaluations, but researchers should note:\n",
   "- Data loss if participant closes browser before export\n",
   "- No guarantee of timestamp accuracy across devices\n",
   "- Mitigation: Use a standardized lab setup with supervised sessions\n",
   "\n",
   "### References\n",
   "- Barkhuus, L., & Rode, J. A. (2007). From mice to men–24 years of evaluation in CHI. *Proc. ACM CHI*.\n",
   "- Dumais, S., et al. (2014). Understanding User Behavior Through Log Data and Analysis. *Ways of Knowing in HCI*, Springer."
  ],
  "outputs": []
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 4.1 Project 1: Deploying Gemini Quest\n",
   "\n",
   "### Quick Start\n",
   "1. Open `deliverables/project1/webapp/index.html` in any modern browser\n",
   "2. Play through the game, making narrative choices\n",
   "3. Click \"Export Logs\" to download interaction data as JSON\n",
   "\n",
   "### For Lab Sessions\n",
   "Run a simple HTTP server to serve the files:\n",
   "```bash\n",
   "cd deliverables/project1/webapp\n",
   "python -m http.server 8080\n",
   "# Open http://localhost:8080 in browser\n",
   "```\n",
   "\n",
   "### Telemetry Data Format\n",
   "The exported JSON contains all interaction events structured for direct analysis pipeline input."
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# PROJECT 1: Verify Deployment Files\n",
   "# ============================================================\n",
   "import webbrowser\n",
   "\n",
   "p1_webapp = P1_DIR / 'webapp' / 'index.html'\n",
   "print(\"=\" * 60)\n",
   "print(\"PROJECT 1: DEPLOYMENT VERIFICATION\")\n",
   "print(\"=\" * 60)\n",
   "\n",
   "if p1_webapp.exists():\n",
   "    file_size = p1_webapp.stat().st_size\n",
   "    print(f\"\\n\\u2713 index.html exists ({file_size:,} bytes)\")\n",
   "    \n",
   "    # Check for key components\n",
   "    with open(p1_webapp, 'r') as f:\n",
   "        content = f.read()\n",
   "    \n",
   "    checks = {\n",
   "        'Telemetry logging': 'telemetryLog' in content,\n",
   "        'Character creation': 'character' in content.lower(),\n",
   "        'Chapter system': 'chapter' in content.lower(),\n",
   "        'Export function': 'export' in content.lower() or 'download' in content.lower(),\n",
   "        'CSS styling': '<style' in content,\n",
   "        'JavaScript': '<script' in content\n",
   "    }\n",
   "    \n",
   "    for check, passed in checks.items():\n",
   "        status = \"\\u2713\" if passed else \"\\u2717\"\n",
   "        print(f\"  {status} {check}\")\n",
   "    \n",
   "    print(f\"\\n\\U0001f4cb To test locally:\")\n",
   "    print(f\"   Option 1: Open the file directly in your browser\")\n",
   "    print(f\"   Option 2: python -m http.server 8080 --directory {P1_DIR / 'webapp'}\")\n",
   "    \n",
   "    # Show telemetry structure\n",
   "    print(f\"\\n\\U0001f4ca Expected telemetry output structure:\")\n",
   "    sample_telemetry = {\n",
   "        \"participant_id\": \"P001\",\n",
   "        \"session_start\": \"2025-01-15T10:30:00.000Z\",\n",
   "        \"session_end\": \"2025-01-15T11:05:00.000Z\",\n",
   "        \"events\": [\n",
   "            {\"timestamp\": \"...\", \"event_type\": \"page_view\", \"page\": \"intro\", \"time_on_page_seconds\": 15},\n",
   "            {\"timestamp\": \"...\", \"event_type\": \"choice_made\", \"page\": \"chapter_1\", \"details\": {\"choice\": \"A\"}}\n",
   "        ]\n",
   "    }\n",
   "    print(json.dumps(sample_telemetry, indent=2))\n",
   "else:\n",
   "    print(f\"\\n\\u2717 index.html not found at {p1_webapp}\")\n",
   "    print(\"  Generate it using the Gemini API (Section 3) or check the deliverables folder.\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 4.2 Project 2: Deploying StudyBuddy\n",
   "\n",
   "### Quick Start (Static Only — No AI Backend)\n",
   "1. Open `deliverables/project2/webapp/index.html` in any modern browser\n",
   "2. The dashboard works standalone with simulated predictions\n",
   "3. Click \"Export Logs\" to download interaction data\n",
   "\n",
   "### Full Deployment (With AutoGluon Backend)\n",
   "1. Install requirements:\n",
   "   ```bash\n",
   "   pip install flask flask-cors autogluon\n",
   "   ```\n",
   "2. Train the model (run Section 3.2 in this notebook)\n",
   "3. Start the Flask API:\n",
   "   ```bash\n",
   "   python deliverables/project2/webapp/app_api.py\n",
   "   ```\n",
   "4. Open `index.html` — it will automatically connect to the API at `localhost:5001`\n",
   "\n",
   "### API Endpoints\n",
   "| Endpoint | Method | Description |\n",
   "|:---|:---|:---|\n",
   "| `/predict` | POST | Send student features, receive predicted score |\n",
   "| `/health` | GET | Check if model is loaded |\n",
   "\n",
   "### Mixed Methods Data Collection Strategy\n",
   "For the usability evaluation, we collect data through **three complementary channels**:\n",
   "\n",
   "1. **Telemetry logs** (automatic) — Behavioral data from prototype interaction\n",
   "2. **Post-test survey** (manual) — Self-reported usability, trust, and perception\n",
   "3. **Think-aloud protocol** (optional) — Verbal protocol for richer qualitative data\n",
   "\n",
   "This triangulation strengthens the validity of our findings (Lazar et al., 2017)."
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# PROJECT 2: Verify Deployment Files\n",
   "# ============================================================\n",
   "print(\"=\" * 60)\n",
   "print(\"PROJECT 2: DEPLOYMENT VERIFICATION\")\n",
   "print(\"=\" * 60)\n",
   "\n",
   "p2_webapp = P2_DIR / 'webapp' / 'index.html'\n",
   "p2_api = P2_DIR / 'webapp' / 'app_api.py'\n",
   "\n",
   "# Check web app\n",
   "if p2_webapp.exists():\n",
   "    file_size = p2_webapp.stat().st_size\n",
   "    print(f\"\\n\\u2713 index.html exists ({file_size:,} bytes)\")\n",
   "    \n",
   "    with open(p2_webapp, 'r') as f:\n",
   "        content = f.read()\n",
   "    \n",
   "    checks = {\n",
   "        'Telemetry logging': 'telemetryLog' in content,\n",
   "        'Dashboard page': 'dashboard' in content.lower(),\n",
   "        'Prediction form': 'predict' in content.lower(),\n",
   "        'Recommendations': 'recommend' in content.lower(),\n",
   "        'Export function': 'export' in content.lower() or 'download' in content.lower(),\n",
   "        'API endpoint reference': 'localhost:5001' in content or 'fetch' in content.lower(),\n",
   "        'CSS styling': '<style' in content,\n",
   "        'JavaScript': '<script' in content\n",
   "    }\n",
   "    \n",
   "    for check, passed in checks.items():\n",
   "        status = \"\\u2713\" if passed else \"\\u2717\"\n",
   "        print(f\"  {status} {check}\")\n",
   "else:\n",
   "    print(f\"\\n\\u2717 index.html not found at {p2_webapp}\")\n",
   "\n",
   "# Check API\n",
   "if p2_api.exists():\n",
   "    api_size = p2_api.stat().st_size\n",
   "    print(f\"\\n\\u2713 app_api.py exists ({api_size:,} bytes)\")\n",
   "    \n",
   "    with open(p2_api, 'r') as f:\n",
   "        api_content = f.read()\n",
   "    \n",
   "    api_checks = {\n",
   "        'Flask app': 'Flask' in api_content,\n",
   "        'CORS enabled': 'CORS' in api_content,\n",
   "        'Predict endpoint': '/predict' in api_content,\n",
   "        'Health endpoint': '/health' in api_content,\n",
   "        'AutoGluon integration': 'autogluon' in api_content.lower() or 'TabularPredictor' in api_content,\n",
   "        'Fallback predictor': 'fallback' in api_content.lower()\n",
   "    }\n",
   "    \n",
   "    for check, passed in api_checks.items():\n",
   "        status = \"\\u2713\" if passed else \"\\u2717\"\n",
   "        print(f\"  {status} {check}\")\n",
   "else:\n",
   "    print(f\"\\n\\u2717 app_api.py not found at {p2_api}\")\n",
   "\n",
   "# Model check\n",
   "model_path = P2_DIR / 'model' / 'AutogluonModels'\n",
   "if model_path.exists():\n",
   "    print(f\"\\n\\u2713 AutoGluon model directory exists: {model_path}\")\n",
   "else:\n",
   "    print(f\"\\n\\u2139 AutoGluon model not yet trained. Run Section 3.2 to train.\")\n",
   "    print(\"  The Flask API includes a fallback linear predictor that works without the model.\")\n",
   "\n",
   "print(f\"\\n\\U0001f4cb Deployment commands:\")\n",
   "print(f\"   Static only: Open {p2_webapp} in browser\")\n",
   "print(f\"   With API:    python {p2_api}\")\n",
   "print(f\"   Test API:    curl -X POST http://localhost:5001/predict -H 'Content-Type: application/json' \\\\\")\n",
   "print(f\"                -d '{{\\\"study_hours_per_week\\\": 15, \\\"attendance_rate\\\": 0.9, \\\"previous_gpa\\\": 3.2}}'\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "---\n",
   "<a id=\"section-5\"></a>\n",
   "# Section 5: User Evaluation\n",
   "\n",
   "## Methodology: Mixed-Methods Usability Evaluation\n",
   "\n",
   "Following established HCI evaluation practices (Lazar et al., 2017), we conduct a **within-subjects usability evaluation** combining:\n",
   "\n",
   "### Standardized Instruments\n",
   "\n",
   "1. **System Usability Scale (SUS)** — Brooke (1996)\n",
   "   - 10-item questionnaire, 5-point Likert scale\n",
   "   - Produces a single score (0-100)\n",
   "   - Industry benchmark: 68 = \"OK\", 80+ = \"Good\"\n",
   "   - Widely used and validated in HCI research\n",
   "\n",
   "2. **User Engagement Scale Short Form (UES-SF)** — O'Brien et al. (2018) [Project 1]\n",
   "   - Subscales: Focused Attention (FA), Perceived Usability (PU), Aesthetic Appeal (AE), Reward Factor (RW)\n",
   "   - 5-point Likert scale\n",
   "\n",
   "3. **Trust in AI Scale** — Adapted from Madsen & Gregor (2000) [Project 2]\n",
   "   - 5 items measuring perceived reliability, competence, and benevolence\n",
   "   - 7-point Likert scale\n",
   "\n",
   "4. **Technology Acceptance Model (TAM)** — Davis (1989) [Project 2]\n",
   "   - Perceived Usefulness (5 items) and Perceived Ease of Use (5 items)\n",
   "   - 7-point Likert scale\n",
   "\n",
   "5. **Custom Scales** (validated via pilot study):\n",
   "   - Narrative Quality (5 items, 7-point) [Project 1]\n",
   "   - AI Perception (5 items, 7-point) [Project 1]\n",
   "   - Immersion (3 items, 7-point) [Project 1]\n",
   "   - Accuracy Perception (3 items, 7-point) [Project 2]\n",
   "   - Privacy Concern (3 items, 7-point) [Project 2]\n",
   "\n",
   "### Qualitative Data Collection\n",
   "- **Open-ended survey items:** Positive feedback, negative feedback, suggestions\n",
   "- **Think-aloud protocol** (optional): Participants verbalize thoughts during interaction\n",
   "- **Behavioral telemetry:** Automatic logging from prototype\n",
   "\n",
   "### Evaluation Protocol\n",
   "1. Informed consent and demographics (5 min)\n",
   "2. Brief tutorial on the prototype (2 min)\n",
   "3. Free exploration / task completion (15-30 min)\n",
   "4. Post-test survey completion (10-15 min)\n",
   "5. Optional debrief interview (5 min)\n",
   "\n",
   "Total session time: ~45 minutes\n",
   "\n",
   "### References\n",
   "- Brooke, J. (1996). SUS: A 'quick and dirty' usability scale. *Usability Evaluation in Industry*.\n",
   "- O'Brien, H. L., Cairns, P., & Hall, M. (2018). A practical approach to measuring user engagement with the refined user engagement scale (UES) and new UES short form. *IJHCS*, 112, 28-39.\n",
   "- Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. *MIS Quarterly*, 13(3), 319-340.\n",
   "- Madsen, M., & Gregor, S. (2000). Measuring human-computer trust. *Proc. Australasian Conference on Information Systems*."
  ],
  "outputs": []
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 5.1 Project 1: Gemini Quest — Post-Test Evaluation\n",
   "\n",
   "### Post-Test Survey Instrument\n",
   "\n",
   "**Part A: System Usability Scale (SUS)** — 5-point Likert (Strongly Disagree to Strongly Agree)\n",
   "1. I think that I would like to use this system frequently.\n",
   "2. I found the system unnecessarily complex. (R)\n",
   "3. I thought the system was easy to use.\n",
   "4. I think that I would need the support of a technical person to use this system. (R)\n",
   "5. I found the various functions in this system were well integrated.\n",
   "6. I thought there was too much inconsistency in this system. (R)\n",
   "7. I would imagine that most people would learn to use this system very quickly.\n",
   "8. I found the system very cumbersome to use. (R)\n",
   "9. I felt very confident using the system.\n",
   "10. I needed to learn a lot of things before I could get going with this system. (R)\n",
   "\n",
   "*(R) = Reverse-coded items*\n",
   "\n",
   "**Part B: User Engagement Scale - Short Form (UES-SF)** — 5-point Likert\n",
   "- Focused Attention (FA): 5 items\n",
   "- Perceived Usability (PU): 3 items\n",
   "- Aesthetic Appeal (AE): 4 items\n",
   "- Reward Factor (RW): 3 items\n",
   "\n",
   "**Part C: Narrative Quality** — 7-point Likert\n",
   "1. The story was engaging and held my attention.\n",
   "2. The characters felt believable and interesting.\n",
   "3. The world-building was rich and immersive.\n",
   "4. My choices felt meaningful and impactful.\n",
   "5. The story had good pacing and flow.\n",
   "\n",
   "**Part D: AI Perception** — 7-point Likert\n",
   "1. The AI-generated content felt natural and coherent.\n",
   "2. I could not distinguish AI content from human-created content.\n",
   "3. The AI enhanced my gaming experience.\n",
   "4. I trust AI to create quality game content.\n",
   "5. I would play more AI-generated games in the future.\n",
   "\n",
   "**Part E: Immersion** — 7-point Likert\n",
   "1. I lost track of time while playing.\n",
   "2. I felt transported to the game world.\n",
   "3. The experience was absorbing.\n",
   "\n",
   "**Part F: Open-Ended Feedback**\n",
   "- What did you enjoy most about the experience? (free text)\n",
   "- What frustrated you or could be improved? (free text)\n",
   "- Any other suggestions for improvement? (free text)"
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# PROJECT 1: Load Post-Test Data and Interaction Logs\n",
   "# ============================================================\n",
   "# Load post-test survey\n",
   "p1_posttest_path = P1_DIR / 'posttest' / 'posttest_survey_responses.csv'\n",
   "p1_logs_path = P1_DIR / 'logs' / 'interaction_logs.json'\n",
   "\n",
   "try:\n",
   "    p1_posttest = pd.read_csv(p1_posttest_path)\n",
   "    print(f\"\\u2713 Loaded P1 post-test data: {p1_posttest.shape[0]} participants, {p1_posttest.shape[1]} columns\")\n",
   "except FileNotFoundError:\n",
   "    print(\"\\u2717 Post-test file not found.\")\n",
   "    p1_posttest = None\n",
   "\n",
   "# Load interaction logs\n",
   "try:\n",
   "    with open(p1_logs_path, 'r') as f:\n",
   "        p1_logs = json.load(f)\n",
   "    print(f\"\\u2713 Loaded P1 interaction logs: {len(p1_logs)} participants\")\n",
   "except FileNotFoundError:\n",
   "    print(\"\\u2717 Interaction logs not found.\")\n",
   "    p1_logs = None\n",
   "\n",
   "# ---- Compute SUS Scores ----\n",
   "if p1_posttest is not None:\n",
   "    sus_cols = [f'sus_q{i}' for i in range(1, 11)]\n",
   "    \n",
   "    def compute_sus(row):\n",
   "        \"\"\"Compute SUS score following Brooke (1996) scoring method.\"\"\"\n",
   "        score = 0\n",
   "        for i in range(1, 11):\n",
   "            val = row[f'sus_q{i}']\n",
   "            if i % 2 == 1:  # Odd items (positive): score - 1\n",
   "                score += (val - 1)\n",
   "            else:  # Even items (negative): 5 - score\n",
   "                score += (5 - val)\n",
   "        return score * 2.5  # Scale to 0-100\n",
   "    \n",
   "    p1_posttest['sus_score'] = p1_posttest.apply(compute_sus, axis=1)\n",
   "    \n",
   "    print(f\"\\n{'='*60}\")\n",
   "    print(\"PROJECT 1: SUS SCORES\")\n",
   "    print(\"=\"*60)\n",
   "    print(f\"  Mean: {p1_posttest['sus_score'].mean():.1f}\")\n",
   "    print(f\"  SD: {p1_posttest['sus_score'].std():.1f}\")\n",
   "    print(f\"  Median: {p1_posttest['sus_score'].median():.1f}\")\n",
   "    print(f\"  Range: [{p1_posttest['sus_score'].min():.1f}, {p1_posttest['sus_score'].max():.1f}]\")\n",
   "    \n",
   "    # SUS Grade (Bangor et al., 2009)\n",
   "    mean_sus = p1_posttest['sus_score'].mean()\n",
   "    if mean_sus >= 80.3:\n",
   "        grade = 'A (Excellent)'\n",
   "    elif mean_sus >= 68:\n",
   "        grade = 'B (Good)'\n",
   "    elif mean_sus >= 51:\n",
   "        grade = 'C (OK)'\n",
   "    else:\n",
   "        grade = 'D/F (Poor)'\n",
   "    print(f\"  SUS Grade: {grade}\")\n",
   "\n",
   "# ---- Summarize Interaction Logs ----\n",
   "if p1_logs is not None:\n",
   "    print(f\"\\n{'='*60}\")\n",
   "    print(\"PROJECT 1: INTERACTION LOG SUMMARY\")\n",
   "    print(\"=\"*60)\n",
   "    \n",
   "    durations = [p['session_duration_seconds'] for p in p1_logs]\n",
   "    clicks = [p['total_clicks'] for p in p1_logs]\n",
   "    pages = [p['total_pages_visited'] for p in p1_logs]\n",
   "    n_events = [len(p['events']) for p in p1_logs]\n",
   "    n_choices = [len(p.get('choices_made', [])) for p in p1_logs]\n",
   "    \n",
   "    print(f\"  Session duration: M={np.mean(durations):.0f}s (SD={np.std(durations):.0f}s)\")\n",
   "    print(f\"  Total clicks: M={np.mean(clicks):.1f} (SD={np.std(clicks):.1f})\")\n",
   "    print(f\"  Pages visited: M={np.mean(pages):.1f} (SD={np.std(pages):.1f})\")\n",
   "    print(f\"  Events logged: M={np.mean(n_events):.1f} (SD={np.std(n_events):.1f})\")\n",
   "    print(f\"  Choices made: M={np.mean(n_choices):.1f} (SD={np.std(n_choices):.1f})\")\n",
   "    \n",
   "    errors = [len(p.get('errors_encountered', [])) for p in p1_logs]\n",
   "    print(f\"  Errors encountered: M={np.mean(errors):.1f} (SD={np.std(errors):.1f})\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 5.2 Project 2: StudyBuddy — Post-Test Evaluation\n",
   "\n",
   "### Post-Test Survey Instrument\n",
   "\n",
   "**Part A: System Usability Scale (SUS)** — Same as Project 1\n",
   "\n",
   "**Part B: Trust in AI** — 7-point Likert (Strongly Disagree to Strongly Agree)\n",
   "1. I believe the system's predictions are reliable.\n",
   "2. I feel confident in the system's recommendations.\n",
   "3. The system seems competent at predicting academic performance.\n",
   "4. I can depend on the system to help me study effectively.\n",
   "5. The system has my best interests in mind.\n",
   "\n",
   "**Part C: Perceived Usefulness (TAM)** — 7-point Likert\n",
   "1. Using this system would improve my academic performance.\n",
   "2. Using this system would increase my study productivity.\n",
   "3. Using this system would make studying more effective.\n",
   "4. I find this system useful for academic planning.\n",
   "5. Using this system would give me greater control over my studies.\n",
   "\n",
   "**Part D: Perceived Ease of Use (TAM)** — 7-point Likert\n",
   "1. Learning to use this system was easy.\n",
   "2. I find it easy to get the system to do what I want.\n",
   "3. The system interface is clear and understandable.\n",
   "4. The system is flexible to interact with.\n",
   "5. It is easy to become skillful at using this system.\n",
   "\n",
   "**Part E: Accuracy Perception** — 7-point Likert\n",
   "1. The predicted scores seem accurate.\n",
   "2. The recommendations are relevant to my situation.\n",
   "3. I would trust these predictions for making study decisions.\n",
   "\n",
   "**Part F: Privacy Concern** — 7-point Likert\n",
   "1. I am concerned about the privacy of my academic data.\n",
   "2. I worry about how my data might be used beyond this system.\n",
   "3. I would want more control over what data the system collects.\n",
   "\n",
   "**Part G: Open-Ended Feedback**\n",
   "- What did you find most useful about StudyBuddy? (free text)\n",
   "- What concerns or frustrations did you experience? (free text)\n",
   "- How would you improve the system? (free text)"
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# PROJECT 2: Load Post-Test Data and Interaction Logs\n",
   "# ============================================================\n",
   "p2_posttest_path = P2_DIR / 'posttest' / 'posttest_survey_responses.csv'\n",
   "p2_logs_path = P2_DIR / 'logs' / 'interaction_logs.json'\n",
   "\n",
   "try:\n",
   "    p2_posttest = pd.read_csv(p2_posttest_path)\n",
   "    print(f\"\\u2713 Loaded P2 post-test data: {p2_posttest.shape[0]} participants, {p2_posttest.shape[1]} columns\")\n",
   "except FileNotFoundError:\n",
   "    print(\"\\u2717 Post-test file not found.\")\n",
   "    p2_posttest = None\n",
   "\n",
   "try:\n",
   "    with open(p2_logs_path, 'r') as f:\n",
   "        p2_logs = json.load(f)\n",
   "    print(f\"\\u2713 Loaded P2 interaction logs: {len(p2_logs)} participants\")\n",
   "except FileNotFoundError:\n",
   "    print(\"\\u2717 Interaction logs not found.\")\n",
   "    p2_logs = None\n",
   "\n",
   "# ---- Compute SUS Scores ----\n",
   "if p2_posttest is not None:\n",
   "    def compute_sus(row):\n",
   "        score = 0\n",
   "        for i in range(1, 11):\n",
   "            val = row[f'sus_q{i}']\n",
   "            if i % 2 == 1:\n",
   "                score += (val - 1)\n",
   "            else:\n",
   "                score += (5 - val)\n",
   "        return score * 2.5\n",
   "    \n",
   "    p2_posttest['sus_score'] = p2_posttest.apply(compute_sus, axis=1)\n",
   "    \n",
   "    print(f\"\\n{'='*60}\")\n",
   "    print(\"PROJECT 2: SUS SCORES\")\n",
   "    print(\"=\"*60)\n",
   "    print(f\"  Mean: {p2_posttest['sus_score'].mean():.1f}\")\n",
   "    print(f\"  SD: {p2_posttest['sus_score'].std():.1f}\")\n",
   "    print(f\"  Median: {p2_posttest['sus_score'].median():.1f}\")\n",
   "    print(f\"  Range: [{p2_posttest['sus_score'].min():.1f}, {p2_posttest['sus_score'].max():.1f}]\")\n",
   "    \n",
   "    mean_sus = p2_posttest['sus_score'].mean()\n",
   "    if mean_sus >= 80.3:\n",
   "        grade = 'A (Excellent)'\n",
   "    elif mean_sus >= 68:\n",
   "        grade = 'B (Good)'\n",
   "    elif mean_sus >= 51:\n",
   "        grade = 'C (OK)'\n",
   "    else:\n",
   "        grade = 'D/F (Poor)'\n",
   "    print(f\"  SUS Grade: {grade}\")\n",
   "\n",
   "    # Trust scores\n",
   "    trust_cols = [f'trust_q{i}' for i in range(1, 6)]\n",
   "    if all(c in p2_posttest.columns for c in trust_cols):\n",
   "        p2_posttest['trust_mean'] = p2_posttest[trust_cols].mean(axis=1)\n",
   "        print(f\"\\n  Trust in AI: M={p2_posttest['trust_mean'].mean():.2f}, SD={p2_posttest['trust_mean'].std():.2f}\")\n",
   "    \n",
   "    # Usefulness scores\n",
   "    use_cols = [f'usefulness_q{i}' for i in range(1, 6)]\n",
   "    if all(c in p2_posttest.columns for c in use_cols):\n",
   "        p2_posttest['usefulness_mean'] = p2_posttest[use_cols].mean(axis=1)\n",
   "        print(f\"  Perceived Usefulness: M={p2_posttest['usefulness_mean'].mean():.2f}, SD={p2_posttest['usefulness_mean'].std():.2f}\")\n",
   "\n",
   "    # Ease of use scores\n",
   "    ease_cols = [f'ease_q{i}' for i in range(1, 6)]\n",
   "    if all(c in p2_posttest.columns for c in ease_cols):\n",
   "        p2_posttest['ease_mean'] = p2_posttest[ease_cols].mean(axis=1)\n",
   "        print(f\"  Perceived Ease of Use: M={p2_posttest['ease_mean'].mean():.2f}, SD={p2_posttest['ease_mean'].std():.2f}\")\n",
   "\n",
   "# ---- Summarize Interaction Logs ----\n",
   "if p2_logs is not None:\n",
   "    print(f\"\\n{'='*60}\")\n",
   "    print(\"PROJECT 2: INTERACTION LOG SUMMARY\")\n",
   "    print(\"=\"*60)\n",
   "    \n",
   "    durations = [p['session_duration_seconds'] for p in p2_logs]\n",
   "    clicks = [p['total_clicks'] for p in p2_logs]\n",
   "    n_events = [len(p['events']) for p in p2_logs]\n",
   "    predictions = [p.get('predictions_viewed', 0) for p in p2_logs]\n",
   "    recommendations = [p.get('recommendations_clicked', 0) for p in p2_logs]\n",
   "    \n",
   "    print(f\"  Session duration: M={np.mean(durations):.0f}s (SD={np.std(durations):.0f}s)\")\n",
   "    print(f\"  Total clicks: M={np.mean(clicks):.1f} (SD={np.std(clicks):.1f})\")\n",
   "    print(f\"  Events logged: M={np.mean(n_events):.1f} (SD={np.std(n_events):.1f})\")\n",
   "    print(f\"  Predictions viewed: M={np.mean(predictions):.1f} (SD={np.std(predictions):.1f})\")\n",
   "    print(f\"  Recommendations clicked: M={np.mean(recommendations):.1f} (SD={np.std(recommendations):.1f})\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "### Generating Dummy Evaluation Data\n",
   "\n",
   "In a live evaluation, the CSV files and interaction logs would come from actual participants. For this workshop, we use pre-generated dummy data that simulates realistic responses:\n",
   "\n",
   "- **Post-test CSVs:** Generated with controlled distributions matching expected patterns\n",
   "  - SUS scores follow approximately normal distribution (P1: M≈72, P2: M≈68)\n",
   "  - Likert responses include realistic variance and inter-item correlations\n",
   "  - Open-ended responses are varied and represent common feedback themes\n",
   "  \n",
   "- **Interaction logs:** Simulated with realistic:\n",
   "  - Session durations (15-60 minutes for P1, 10-45 minutes for P2)\n",
   "  - Event sequences following logical page navigation patterns\n",
   "  - Feature usage patterns matching expected prototype exploration\n",
   "  - Error occurrences at realistic low rates\n",
   "\n",
   "- **Qualitative coded data:** Pre-coded with dual-coder simulation\n",
   "  - Two \"LLM coders\" independently assigned codes\n",
   "  - ~83-85% inter-coder agreement (realistic for open coding)\n",
   "  - Enables computing inter-rater reliability in Section 6\n",
   "\n",
   "> **Validity Note:** While dummy data allows demonstrating the full pipeline, all statistical results should be interpreted as illustrative only. In a real study, actual participant data would replace these files at this step."
  ],
  "outputs": []
 }
]