[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "---\n",
   "<a id=\"section-6\"></a>\n",
   "# Section 6: Analyses\n",
   "\n",
   "## Mixed-Methods Analysis Framework\n",
   "\n",
   "Following Creswell & Clark (2017), our analysis proceeds in three phases:\n",
   "\n",
   "### Phase 1: Quantitative Analysis\n",
   "1. **Descriptive statistics** — Means, SDs, distributions for all scales\n",
   "2. **Reliability analysis** — Cronbach’s alpha for multi-item scales\n",
   "3. **Normality testing** — Shapiro-Wilk test to determine parametric vs. non-parametric tests\n",
   "4. **Hypothesis testing** — t-tests, ANOVA, correlation analyses\n",
   "5. **Effect sizes** — Cohen’s d, eta-squared, correlation coefficients\n",
   "6. **Behavioral metrics** — Telemetry-derived engagement measures\n",
   "\n",
   "### Phase 2: Qualitative Analysis\n",
   "1. **Open coding** — Identifying initial codes from open-ended responses\n",
   "2. **Axial coding** — Grouping codes into themes\n",
   "3. **Inter-rater reliability** — Cohen’s Kappa and Krippendorff’s Alpha between coders\n",
   "4. **Theme frequency analysis** — Quantifying qualitative patterns\n",
   "\n",
   "### Phase 3: Integration (Mixed Methods)\n",
   "1. **Joint display tables** — Side-by-side quantitative + qualitative findings\n",
   "2. **Convergence analysis** — Where do quantitative and qualitative findings agree/diverge?\n",
   "3. **Complementarity** — How qualitative data enriches quantitative findings\n",
   "\n",
   "### Important: LLM-Based Qualitative Analysis\n",
   "In this workshop, we demonstrate using LLMs (Gemini) for qualitative coding. This approach has important methodological implications:\n",
   "\n",
   "**Potential Benefits:**\n",
   "- Speed and scalability for large datasets\n",
   "- Consistency in applying coding schemes\n",
   "- Reproducibility of coding process\n",
   "\n",
   "**Validity Concerns:**\n",
   "- LLMs may miss nuanced, context-dependent meanings (Tai et al., 2024)\n",
   "- Risk of systematic bias in code assignment\n",
   "- Cannot fully replace human interpretive judgment\n",
   "\n",
   "**Mitigation Strategies:**\n",
   "1. Use LLM coding as a **starting point**, not final analysis\n",
   "2. Have **human coders verify** a random sample (≥20%)\n",
   "3. Report **inter-rater reliability** between LLM and human coders\n",
   "4. Acknowledge automation in the **limitations section**\n",
   "5. Use multiple LLM “coders” with different prompts to simulate independent coding\n",
   "\n",
   "**References:**\n",
   "- Tai, R. H., et al. (2024). An Examination of the Use of Large Language Models to Aid Analysis of Textual Data. *International Journal of Qualitative Methods*.\n",
   "- Xiao, Z., et al. (2023). Supporting Qualitative Analysis with Large Language Models. *Proc. ACM CHI*."
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# SECTION 6.1: QUANTITATIVE ANALYSIS — PROJECT 1 (Gemini Quest)\n",
   "# ============================================================\n",
   "print(\"=\" * 70)\n",
   "print(\"PROJECT 1: QUANTITATIVE ANALYSIS\")\n",
   "print(\"=\" * 70)\n",
   "\n",
   "# --- 6.1.1: Descriptive Statistics ---\n",
   "print(\"\\n\" + \"─\" * 70)\n",
   "print(\"6.1.1 DESCRIPTIVE STATISTICS\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "# SUS\n",
   "print(f\"\\nSystem Usability Scale (SUS):\")\n",
   "print(f\"  N = {len(p1_posttest)}\")\n",
   "print(f\"  Mean = {p1_posttest['sus_score'].mean():.2f}\")\n",
   "print(f\"  SD = {p1_posttest['sus_score'].std():.2f}\")\n",
   "print(f\"  Median = {p1_posttest['sus_score'].median():.2f}\")\n",
   "print(f\"  95% CI = [{p1_posttest['sus_score'].mean() - 1.96*p1_posttest['sus_score'].std()/np.sqrt(len(p1_posttest)):.2f}, \"\n",
   "      f\"{p1_posttest['sus_score'].mean() + 1.96*p1_posttest['sus_score'].std()/np.sqrt(len(p1_posttest)):.2f}]\")\n",
   "\n",
   "# UES-SF Subscales\n",
   "print(f\"\\nUser Engagement Scale - Short Form (UES-SF):\")\n",
   "ues_subscales = {\n",
   "    'Focused Attention (FA)': [f'engagement_fa{i}' for i in range(1, 6)],\n",
   "    'Perceived Usability (PU)': [f'engagement_pu{i}' for i in range(1, 4)],\n",
   "    'Aesthetic Appeal (AE)': [f'engagement_ae{i}' for i in range(1, 5)],\n",
   "    'Reward Factor (RW)': [f'engagement_rw{i}' for i in range(1, 4)]\n",
   "}\n",
   "\n",
   "for name, cols in ues_subscales.items():\n",
   "    valid_cols = [c for c in cols if c in p1_posttest.columns]\n",
   "    if valid_cols:\n",
   "        subscale_mean = p1_posttest[valid_cols].mean(axis=1)\n",
   "        print(f\"  {name}: M={subscale_mean.mean():.2f}, SD={subscale_mean.std():.2f}\")\n",
   "\n",
   "# Narrative Quality\n",
   "nq_cols = [f'narrative_quality_q{i}' for i in range(1, 6)]\n",
   "valid_nq = [c for c in nq_cols if c in p1_posttest.columns]\n",
   "if valid_nq:\n",
   "    p1_posttest['nq_mean'] = p1_posttest[valid_nq].mean(axis=1)\n",
   "    print(f\"\\nNarrative Quality: M={p1_posttest['nq_mean'].mean():.2f}, SD={p1_posttest['nq_mean'].std():.2f}\")\n",
   "\n",
   "# AI Perception\n",
   "ai_cols = [f'ai_perception_q{i}' for i in range(1, 6)]\n",
   "valid_ai = [c for c in ai_cols if c in p1_posttest.columns]\n",
   "if valid_ai:\n",
   "    p1_posttest['ai_mean'] = p1_posttest[valid_ai].mean(axis=1)\n",
   "    print(f\"AI Perception: M={p1_posttest['ai_mean'].mean():.2f}, SD={p1_posttest['ai_mean'].std():.2f}\")\n",
   "\n",
   "# Immersion\n",
   "imm_cols = [f'immersion_q{i}' for i in range(1, 4)]\n",
   "valid_imm = [c for c in imm_cols if c in p1_posttest.columns]\n",
   "if valid_imm:\n",
   "    p1_posttest['immersion_mean'] = p1_posttest[valid_imm].mean(axis=1)\n",
   "    print(f\"Immersion: M={p1_posttest['immersion_mean'].mean():.2f}, SD={p1_posttest['immersion_mean'].std():.2f}\")\n",
   "\n",
   "# --- 6.1.2: Reliability Analysis (Cronbach's Alpha) ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"6.1.2 RELIABILITY ANALYSIS (Cronbach's Alpha)\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "def cronbachs_alpha(df):\n",
   "    \"\"\"Compute Cronbach's alpha for a set of items.\"\"\"\n",
   "    item_vars = df.var(axis=0, ddof=1)\n",
   "    total_var = df.sum(axis=1).var(ddof=1)\n",
   "    n_items = df.shape[1]\n",
   "    if total_var == 0:\n",
   "        return 0\n",
   "    return (n_items / (n_items - 1)) * (1 - item_vars.sum() / total_var)\n",
   "\n",
   "scales = {\n",
   "    'SUS': [f'sus_q{i}' for i in range(1, 11)],\n",
   "    'UES-SF (Full)': [c for c in p1_posttest.columns if c.startswith('engagement_')],\n",
   "    'Narrative Quality': valid_nq,\n",
   "    'AI Perception': valid_ai,\n",
   "    'Immersion': valid_imm\n",
   "}\n",
   "\n",
   "for name, cols in scales.items():\n",
   "    valid = [c for c in cols if c in p1_posttest.columns]\n",
   "    if len(valid) >= 2:\n",
   "        alpha = cronbachs_alpha(p1_posttest[valid])\n",
   "        quality = \"Excellent\" if alpha >= 0.9 else \"Good\" if alpha >= 0.8 else \"Acceptable\" if alpha >= 0.7 else \"Questionable\" if alpha >= 0.6 else \"Poor\"\n",
   "        print(f\"  {name}: α = {alpha:.3f} ({quality})\")\n",
   "\n",
   "# --- 6.1.3: Normality Testing ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"6.1.3 NORMALITY TESTING (Shapiro-Wilk)\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "normality_vars = {\n",
   "    'SUS Score': p1_posttest['sus_score'],\n",
   "    'Narrative Quality': p1_posttest.get('nq_mean'),\n",
   "    'AI Perception': p1_posttest.get('ai_mean'),\n",
   "    'Immersion': p1_posttest.get('immersion_mean')\n",
   "}\n",
   "\n",
   "normality_results = {}\n",
   "for name, data in normality_vars.items():\n",
   "    if data is not None:\n",
   "        stat, p_val = shapiro(data)\n",
   "        is_normal = p_val > 0.05\n",
   "        normality_results[name] = is_normal\n",
   "        print(f\"  {name}: W={stat:.4f}, p={p_val:.4f} {'(Normal)' if is_normal else '(Non-normal)'}\")\n",
   "\n",
   "# --- 6.1.4: Hypothesis Testing ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"6.1.4 HYPOTHESIS TESTING\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "# H1: Compare SUS scores between groups (simulate two conditions)\n",
   "# Split participants into two groups for demonstration\n",
   "np.random.seed(42)\n",
   "group_labels = np.random.choice(['preferences_integrated', 'generic'], size=len(p1_posttest))\n",
   "p1_posttest['condition'] = group_labels\n",
   "\n",
   "group_a = p1_posttest[p1_posttest['condition'] == 'preferences_integrated']['sus_score']\n",
   "group_b = p1_posttest[p1_posttest['condition'] == 'generic']['sus_score']\n",
   "\n",
   "print(f\"\\nH1: SUS scores — Preferences Integrated vs. Generic\")\n",
   "print(f\"  Group A (Integrated): M={group_a.mean():.2f}, SD={group_a.std():.2f}, n={len(group_a)}\")\n",
   "print(f\"  Group B (Generic): M={group_b.mean():.2f}, SD={group_b.std():.2f}, n={len(group_b)}\")\n",
   "\n",
   "t_stat, p_val = stats.ttest_ind(group_a, group_b)\n",
   "cohens_d = (group_a.mean() - group_b.mean()) / np.sqrt((group_a.std()**2 + group_b.std()**2) / 2)\n",
   "print(f\"  t({len(group_a)+len(group_b)-2}) = {t_stat:.3f}, p = {p_val:.4f}\")\n",
   "print(f\"  Cohen's d = {cohens_d:.3f}\")\n",
   "print(f\"  Result: {'Significant' if p_val < 0.05 else 'Not significant'} at α = 0.05\")\n",
   "\n",
   "# H2: Correlation between narrative quality and engagement\n",
   "if 'nq_mean' in p1_posttest.columns:\n",
   "    # Overall engagement\n",
   "    ues_all = [c for c in p1_posttest.columns if c.startswith('engagement_')]\n",
   "    if ues_all:\n",
   "        p1_posttest['ues_overall'] = p1_posttest[ues_all].mean(axis=1)\n",
   "        r, p_val = stats.pearsonr(p1_posttest['nq_mean'], p1_posttest['ues_overall'])\n",
   "        print(f\"\\nH2: Correlation — Narrative Quality × Engagement\")\n",
   "        print(f\"  Pearson r = {r:.3f}, p = {p_val:.4f}\")\n",
   "        print(f\"  Result: {'Significant' if p_val < 0.05 else 'Not significant'} (threshold: r > 0.3)\")\n",
   "\n",
   "# H3: Immersion difference (simulate aware vs unaware)\n",
   "if 'immersion_mean' in p1_posttest.columns:\n",
   "    aware = p1_posttest['immersion_mean'].iloc[:20]\n",
   "    unaware = p1_posttest['immersion_mean'].iloc[20:]\n",
   "    t_stat, p_val = stats.ttest_ind(aware, unaware)\n",
   "    d = (aware.mean() - unaware.mean()) / np.sqrt((aware.std()**2 + unaware.std()**2) / 2)\n",
   "    print(f\"\\nH3: Immersion — AI Aware vs. Unaware\")\n",
   "    print(f\"  Aware: M={aware.mean():.2f}, SD={aware.std():.2f}\")\n",
   "    print(f\"  Unaware: M={unaware.mean():.2f}, SD={unaware.std():.2f}\")\n",
   "    print(f\"  t({len(aware)+len(unaware)-2}) = {t_stat:.3f}, p = {p_val:.4f}\")\n",
   "    print(f\"  Cohen's d = {d:.3f}\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# PROJECT 1: VISUALIZATION\n",
   "# ============================================================\n",
   "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
   "fig.suptitle('Project 1: Gemini Quest — Evaluation Results (N=40)', fontsize=16, fontweight='bold')\n",
   "\n",
   "# SUS Score Distribution\n",
   "axes[0, 0].hist(p1_posttest['sus_score'], bins=12, color='#5C6BC0', alpha=0.7, edgecolor='white')\n",
   "axes[0, 0].axvline(x=68, color='red', linestyle='--', label='Benchmark (68)')\n",
   "axes[0, 0].axvline(x=p1_posttest['sus_score'].mean(), color='green', linestyle='-', label=f'Mean ({p1_posttest[\"sus_score\"].mean():.1f})')\n",
   "axes[0, 0].set_title('SUS Score Distribution')\n",
   "axes[0, 0].set_xlabel('SUS Score')\n",
   "axes[0, 0].set_ylabel('Frequency')\n",
   "axes[0, 0].legend(fontsize=9)\n",
   "\n",
   "# UES Subscale Comparison\n",
   "ues_data = {}\n",
   "for name, cols in ues_subscales.items():\n",
   "    valid = [c for c in cols if c in p1_posttest.columns]\n",
   "    if valid:\n",
   "        ues_data[name.split('(')[0].strip()] = p1_posttest[valid].mean(axis=1)\n",
   "\n",
   "if ues_data:\n",
   "    ues_df = pd.DataFrame(ues_data)\n",
   "    bp = axes[0, 1].boxplot([ues_df[col] for col in ues_df.columns], labels=ues_df.columns, patch_artist=True)\n",
   "    colors = ['#42A5F5', '#66BB6A', '#FFA726', '#EF5350']\n",
   "    for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
   "        patch.set_facecolor(color)\n",
   "        patch.set_alpha(0.7)\n",
   "    axes[0, 1].set_title('UES-SF Subscales')\n",
   "    axes[0, 1].set_ylabel('Score (1-5)')\n",
   "\n",
   "# Custom Scale Means\n",
   "custom_scales = {}\n",
   "if 'nq_mean' in p1_posttest.columns:\n",
   "    custom_scales['Narrative\\nQuality'] = p1_posttest['nq_mean']\n",
   "if 'ai_mean' in p1_posttest.columns:\n",
   "    custom_scales['AI\\nPerception'] = p1_posttest['ai_mean']\n",
   "if 'immersion_mean' in p1_posttest.columns:\n",
   "    custom_scales['Immersion'] = p1_posttest['immersion_mean']\n",
   "\n",
   "if custom_scales:\n",
   "    means = [v.mean() for v in custom_scales.values()]\n",
   "    sds = [v.std() for v in custom_scales.values()]\n",
   "    bars = axes[0, 2].bar(list(custom_scales.keys()), means, yerr=sds, capsize=5,\n",
   "                           color=['#AB47BC', '#26A69A', '#EC407A'], alpha=0.8)\n",
   "    axes[0, 2].set_title('Custom Scale Ratings')\n",
   "    axes[0, 2].set_ylabel('Mean (1-7)')\n",
   "    axes[0, 2].set_ylim(1, 7)\n",
   "    axes[0, 2].axhline(y=4, color='gray', linestyle='--', alpha=0.5)\n",
   "\n",
   "# Correlation Heatmap\n",
   "if 'nq_mean' in p1_posttest.columns and 'ues_overall' in p1_posttest.columns:\n",
   "    corr_cols = ['sus_score', 'nq_mean', 'ai_mean', 'immersion_mean', 'ues_overall']\n",
   "    valid_corr = [c for c in corr_cols if c in p1_posttest.columns]\n",
   "    corr_matrix = p1_posttest[valid_corr].corr()\n",
   "    labels_map = {'sus_score': 'SUS', 'nq_mean': 'Narrative', 'ai_mean': 'AI Percep.',\n",
   "                  'immersion_mean': 'Immersion', 'ues_overall': 'Engagement'}\n",
   "    corr_matrix.columns = [labels_map.get(c, c) for c in corr_matrix.columns]\n",
   "    corr_matrix.index = [labels_map.get(c, c) for c in corr_matrix.index]\n",
   "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r', center=0,\n",
   "                ax=axes[1, 0], vmin=-1, vmax=1, square=True)\n",
   "    axes[1, 0].set_title('Correlation Matrix')\n",
   "\n",
   "# Behavioral Data\n",
   "if p1_logs:\n",
   "    durations_min = [p['session_duration_seconds'] / 60 for p in p1_logs]\n",
   "    axes[1, 1].hist(durations_min, bins=10, color='#78909C', alpha=0.7, edgecolor='white')\n",
   "    axes[1, 1].set_title('Session Duration Distribution')\n",
   "    axes[1, 1].set_xlabel('Duration (minutes)')\n",
   "    axes[1, 1].set_ylabel('Frequency')\n",
   "\n",
   "# SUS by condition\n",
   "if 'condition' in p1_posttest.columns:\n",
   "    conditions = p1_posttest.groupby('condition')['sus_score']\n",
   "    cond_names = list(conditions.groups.keys())\n",
   "    cond_data = [conditions.get_group(c) for c in cond_names]\n",
   "    bp2 = axes[1, 2].boxplot(cond_data, labels=[c.replace('_', '\\n') for c in cond_names], patch_artist=True)\n",
   "    bp2['boxes'][0].set_facecolor('#66BB6A')\n",
   "    bp2['boxes'][0].set_alpha(0.7)\n",
   "    if len(bp2['boxes']) > 1:\n",
   "        bp2['boxes'][1].set_facecolor('#EF5350')\n",
   "        bp2['boxes'][1].set_alpha(0.7)\n",
   "    axes[1, 2].set_title('SUS Score by Condition')\n",
   "    axes[1, 2].set_ylabel('SUS Score')\n",
   "    axes[1, 2].axhline(y=68, color='gray', linestyle='--', alpha=0.5)\n",
   "\n",
   "plt.tight_layout()\n",
   "plt.savefig(str(DELIVERABLES / 'report' / 'p1_evaluation_results.png'), dpi=150, bbox_inches='tight')\n",
   "plt.show()\n",
   "print(\"✓ Figure saved to deliverables/report/p1_evaluation_results.png\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# SECTION 6.2: QUANTITATIVE ANALYSIS — PROJECT 2 (StudyBuddy)\n",
   "# ============================================================\n",
   "print(\"=\" * 70)\n",
   "print(\"PROJECT 2: QUANTITATIVE ANALYSIS\")\n",
   "print(\"=\" * 70)\n",
   "\n",
   "# --- Descriptive Statistics ---\n",
   "print(\"\\n\" + \"─\" * 70)\n",
   "print(\"6.2.1 DESCRIPTIVE STATISTICS\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "print(f\"\\nSystem Usability Scale (SUS):\")\n",
   "print(f\"  N = {len(p2_posttest)}\")\n",
   "print(f\"  Mean = {p2_posttest['sus_score'].mean():.2f}\")\n",
   "print(f\"  SD = {p2_posttest['sus_score'].std():.2f}\")\n",
   "print(f\"  95% CI = [{p2_posttest['sus_score'].mean() - 1.96*p2_posttest['sus_score'].std()/np.sqrt(len(p2_posttest)):.2f}, \"\n",
   "      f\"{p2_posttest['sus_score'].mean() + 1.96*p2_posttest['sus_score'].std()/np.sqrt(len(p2_posttest)):.2f}]\")\n",
   "\n",
   "for scale_name, col_name in [('Trust in AI', 'trust_mean'), ('Perceived Usefulness', 'usefulness_mean'), \n",
   "                               ('Perceived Ease of Use', 'ease_mean')]:\n",
   "    if col_name in p2_posttest.columns:\n",
   "        print(f\"\\n{scale_name}:\")\n",
   "        print(f\"  Mean = {p2_posttest[col_name].mean():.2f}, SD = {p2_posttest[col_name].std():.2f}\")\n",
   "\n",
   "# Accuracy perception\n",
   "acc_cols = [f'accuracy_perception_q{i}' for i in range(1, 4)]\n",
   "valid_acc = [c for c in acc_cols if c in p2_posttest.columns]\n",
   "if valid_acc:\n",
   "    p2_posttest['accuracy_mean'] = p2_posttest[valid_acc].mean(axis=1)\n",
   "    print(f\"\\nAccuracy Perception: M={p2_posttest['accuracy_mean'].mean():.2f}, SD={p2_posttest['accuracy_mean'].std():.2f}\")\n",
   "\n",
   "# Privacy concern\n",
   "priv_cols = [f'privacy_concern_q{i}' for i in range(1, 4)]\n",
   "valid_priv = [c for c in priv_cols if c in p2_posttest.columns]\n",
   "if valid_priv:\n",
   "    p2_posttest['privacy_mean'] = p2_posttest[valid_priv].mean(axis=1)\n",
   "    print(f\"Privacy Concern: M={p2_posttest['privacy_mean'].mean():.2f}, SD={p2_posttest['privacy_mean'].std():.2f}\")\n",
   "\n",
   "# --- Reliability ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"6.2.2 RELIABILITY ANALYSIS\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "p2_scales = {\n",
   "    'SUS': [f'sus_q{i}' for i in range(1, 11)],\n",
   "    'Trust in AI': [f'trust_q{i}' for i in range(1, 6)],\n",
   "    'Perceived Usefulness': [f'usefulness_q{i}' for i in range(1, 6)],\n",
   "    'Perceived Ease of Use': [f'ease_q{i}' for i in range(1, 6)],\n",
   "    'Accuracy Perception': valid_acc,\n",
   "    'Privacy Concern': valid_priv\n",
   "}\n",
   "\n",
   "for name, cols in p2_scales.items():\n",
   "    valid = [c for c in cols if c in p2_posttest.columns]\n",
   "    if len(valid) >= 2:\n",
   "        alpha = cronbachs_alpha(p2_posttest[valid])\n",
   "        quality = \"Excellent\" if alpha >= 0.9 else \"Good\" if alpha >= 0.8 else \"Acceptable\" if alpha >= 0.7 else \"Questionable\" if alpha >= 0.6 else \"Poor\"\n",
   "        print(f\"  {name}: α = {alpha:.3f} ({quality})\")\n",
   "\n",
   "# --- Normality ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"6.2.3 NORMALITY TESTING\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "for name, col in [('SUS', 'sus_score'), ('Trust', 'trust_mean'), \n",
   "                   ('Usefulness', 'usefulness_mean'), ('Ease', 'ease_mean')]:\n",
   "    if col in p2_posttest.columns:\n",
   "        stat, p_val = shapiro(p2_posttest[col])\n",
   "        print(f\"  {name}: W={stat:.4f}, p={p_val:.4f} {'(Normal)' if p_val > 0.05 else '(Non-normal)'}\")\n",
   "\n",
   "# --- Hypothesis Testing ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"6.2.4 HYPOTHESIS TESTING\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "# H1: Trust across explanation levels (simulate 3 groups)\n",
   "np.random.seed(43)\n",
   "p2_posttest['explanation_level'] = np.random.choice(['none', 'simple', 'detailed'], size=len(p2_posttest))\n",
   "\n",
   "print(f\"\\nH1: Trust by Explanation Level (One-way ANOVA)\")\n",
   "groups = p2_posttest.groupby('explanation_level')['trust_mean']\n",
   "for name, group in groups:\n",
   "    print(f\"  {name}: M={group.mean():.2f}, SD={group.std():.2f}, n={len(group)}\")\n",
   "\n",
   "group_data = [group.values for _, group in groups]\n",
   "f_stat, p_val = stats.f_oneway(*group_data)\n",
   "# Eta-squared\n",
   "ss_between = sum(len(g) * (g.mean() - p2_posttest['trust_mean'].mean())**2 for g in group_data)\n",
   "ss_total = sum((p2_posttest['trust_mean'] - p2_posttest['trust_mean'].mean())**2)\n",
   "eta_sq = ss_between / ss_total if ss_total > 0 else 0\n",
   "print(f\"  F({len(group_data)-1}, {len(p2_posttest)-len(group_data)}) = {f_stat:.3f}, p = {p_val:.4f}\")\n",
   "print(f\"  η² = {eta_sq:.3f}\")\n",
   "\n",
   "# H2: Usefulness × usage correlation\n",
   "if p2_logs:\n",
   "    usage_counts = {p['participant_id']: len(p['events']) for p in p2_logs}\n",
   "    p2_posttest['usage_count'] = p2_posttest['participant_id'].map(usage_counts).fillna(0)\n",
   "    if 'usefulness_mean' in p2_posttest.columns:\n",
   "        rho, p_val = stats.spearmanr(p2_posttest['usefulness_mean'], p2_posttest['usage_count'])\n",
   "        print(f\"\\nH2: Usefulness × Usage (Spearman)\")\n",
   "        print(f\"  ρ = {rho:.3f}, p = {p_val:.4f}\")\n",
   "\n",
   "# H3: Privacy concern × Trust\n",
   "if 'privacy_mean' in p2_posttest.columns and 'trust_mean' in p2_posttest.columns:\n",
   "    r, p_val = stats.pearsonr(p2_posttest['privacy_mean'], p2_posttest['trust_mean'])\n",
   "    print(f\"\\nH3: Privacy Concern × Trust (Pearson)\")\n",
   "    print(f\"  r = {r:.3f}, p = {p_val:.4f}\")\n",
   "    print(f\"  Direction: {'Negative (as expected)' if r < 0 else 'Positive (unexpected)'}\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# PROJECT 2: VISUALIZATION\n",
   "# ============================================================\n",
   "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
   "fig.suptitle('Project 2: StudyBuddy — Evaluation Results (N=40)', fontsize=16, fontweight='bold')\n",
   "\n",
   "# SUS Distribution\n",
   "axes[0, 0].hist(p2_posttest['sus_score'], bins=12, color='#26A69A', alpha=0.7, edgecolor='white')\n",
   "axes[0, 0].axvline(x=68, color='red', linestyle='--', label='Benchmark (68)')\n",
   "axes[0, 0].axvline(x=p2_posttest['sus_score'].mean(), color='green', linestyle='-', \n",
   "                     label=f'Mean ({p2_posttest[\"sus_score\"].mean():.1f})')\n",
   "axes[0, 0].set_title('SUS Score Distribution')\n",
   "axes[0, 0].set_xlabel('SUS Score')\n",
   "axes[0, 0].legend(fontsize=9)\n",
   "\n",
   "# Scale Comparison\n",
   "scale_data = {}\n",
   "for name, col in [('Trust', 'trust_mean'), ('Usefulness', 'usefulness_mean'), \n",
   "                   ('Ease of Use', 'ease_mean'), ('Accuracy', 'accuracy_mean'), ('Privacy', 'privacy_mean')]:\n",
   "    if col in p2_posttest.columns:\n",
   "        scale_data[name] = p2_posttest[col]\n",
   "\n",
   "if scale_data:\n",
   "    means = [v.mean() for v in scale_data.values()]\n",
   "    sds = [v.std() for v in scale_data.values()]\n",
   "    colors = ['#42A5F5', '#66BB6A', '#FFA726', '#AB47BC', '#EF5350']\n",
   "    axes[0, 1].bar(list(scale_data.keys()), means, yerr=sds, capsize=5,\n",
   "                    color=colors[:len(means)], alpha=0.8)\n",
   "    axes[0, 1].set_title('Scale Ratings')\n",
   "    axes[0, 1].set_ylabel('Mean (1-7)')\n",
   "    axes[0, 1].set_ylim(1, 7)\n",
   "    axes[0, 1].axhline(y=4, color='gray', linestyle='--', alpha=0.5)\n",
   "    axes[0, 1].tick_params(axis='x', rotation=15)\n",
   "\n",
   "# Correlation heatmap\n",
   "corr_cols_p2 = ['sus_score', 'trust_mean', 'usefulness_mean', 'ease_mean']\n",
   "valid_corr_p2 = [c for c in corr_cols_p2 if c in p2_posttest.columns]\n",
   "if len(valid_corr_p2) >= 2:\n",
   "    corr_m = p2_posttest[valid_corr_p2].corr()\n",
   "    label_map = {'sus_score': 'SUS', 'trust_mean': 'Trust', 'usefulness_mean': 'Useful',\n",
   "                 'ease_mean': 'Ease', 'accuracy_mean': 'Accuracy', 'privacy_mean': 'Privacy'}\n",
   "    corr_m.columns = [label_map.get(c, c) for c in corr_m.columns]\n",
   "    corr_m.index = [label_map.get(c, c) for c in corr_m.index]\n",
   "    sns.heatmap(corr_m, annot=True, fmt='.2f', cmap='RdYlBu_r', center=0,\n",
   "                ax=axes[0, 2], vmin=-1, vmax=1, square=True)\n",
   "    axes[0, 2].set_title('Correlation Matrix')\n",
   "\n",
   "# Trust by explanation level\n",
   "if 'explanation_level' in p2_posttest.columns:\n",
   "    groups = p2_posttest.groupby('explanation_level')['trust_mean']\n",
   "    group_names = list(groups.groups.keys())\n",
   "    group_vals = [groups.get_group(n) for n in group_names]\n",
   "    bp = axes[1, 0].boxplot(group_vals, labels=group_names, patch_artist=True)\n",
   "    box_colors = ['#EF5350', '#FFA726', '#66BB6A']\n",
   "    for patch, color in zip(bp['boxes'], box_colors[:len(bp['boxes'])]):\n",
   "        patch.set_facecolor(color)\n",
   "        patch.set_alpha(0.7)\n",
   "    axes[1, 0].set_title('Trust by Explanation Level')\n",
   "    axes[1, 0].set_ylabel('Trust Score (1-7)')\n",
   "\n",
   "# Session durations\n",
   "if p2_logs:\n",
   "    dur = [p['session_duration_seconds'] / 60 for p in p2_logs]\n",
   "    axes[1, 1].hist(dur, bins=10, color='#78909C', alpha=0.7, edgecolor='white')\n",
   "    axes[1, 1].set_title('Session Duration')\n",
   "    axes[1, 1].set_xlabel('Duration (minutes)')\n",
   "\n",
   "# Privacy vs Trust scatter\n",
   "if 'privacy_mean' in p2_posttest.columns and 'trust_mean' in p2_posttest.columns:\n",
   "    axes[1, 2].scatter(p2_posttest['privacy_mean'], p2_posttest['trust_mean'], \n",
   "                        alpha=0.6, color='#5C6BC0', s=50)\n",
   "    z = np.polyfit(p2_posttest['privacy_mean'], p2_posttest['trust_mean'], 1)\n",
   "    p_line = np.poly1d(z)\n",
   "    x_line = np.linspace(p2_posttest['privacy_mean'].min(), p2_posttest['privacy_mean'].max(), 100)\n",
   "    axes[1, 2].plot(x_line, p_line(x_line), 'r--', alpha=0.8)\n",
   "    axes[1, 2].set_title('Privacy Concern vs. Trust')\n",
   "    axes[1, 2].set_xlabel('Privacy Concern (1-7)')\n",
   "    axes[1, 2].set_ylabel('Trust in AI (1-7)')\n",
   "\n",
   "plt.tight_layout()\n",
   "plt.savefig(str(DELIVERABLES / 'report' / 'p2_evaluation_results.png'), dpi=150, bbox_inches='tight')\n",
   "plt.show()\n",
   "print(\"✓ Figure saved to deliverables/report/p2_evaluation_results.png\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## 6.3 Qualitative Analysis\n",
   "\n",
   "### Approach: LLM-Assisted Thematic Analysis\n",
   "\n",
   "We follow Braun & Clarke's (2006) **reflexive thematic analysis** framework, augmented with LLM coding:\n",
   "\n",
   "1. **Familiarization** — Read all open-ended responses\n",
   "2. **Initial coding** — LLM generates initial codes (two independent \"coders\")\n",
   "3. **Theme development** — Group codes into higher-level themes\n",
   "4. **Review** — Verify themes against data\n",
   "5. **Define and name** — Finalize theme definitions\n",
   "6. **Report** — Integrate with quantitative findings\n",
   "\n",
   "### Inter-Rater Reliability\n",
   "We compute two measures:\n",
   "- **Cohen's Kappa (κ):** Agreement between two coders adjusted for chance\n",
   "  - κ < 0.20: Slight, 0.21-0.40: Fair, 0.41-0.60: Moderate, 0.61-0.80: Substantial, 0.81-1.00: Almost perfect\n",
   "- **Krippendorff's Alpha (α):** More robust, handles multiple coders and missing data\n",
   "  - α ≥ 0.80: Reliable, 0.667-0.80: Tentative conclusions, < 0.667: Unreliable\n",
   "\n",
   "### Validity Warning for LLM-Based Coding\n",
   "> **Critical Limitation:** Using LLMs for qualitative coding raises validity concerns:\n",
   "> - LLMs may impose **systematic biases** in code assignment\n",
   "> - LLM \"coders\" are **not truly independent** (same underlying model)\n",
   "> - **Mitigation:** At minimum, a human coder should verify 20-30% of LLM-assigned codes\n",
   "> - In a real study, report LLM coding as a **preliminary/exploratory** analysis\n",
   "> - Full validity requires human coder involvement (Xiao et al., 2023)\n",
   "\n",
   "### References\n",
   "- Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. *Qualitative Research in Psychology*, 3(2), 77-101.\n",
   "- Cohen, J. (1960). A coefficient of agreement for nominal scales. *Educational and Psychological Measurement*.\n",
   "- Krippendorff, K. (2011). Computing Krippendorff's Alpha-Reliability."
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# SECTION 6.3: QUALITATIVE ANALYSIS\n",
   "# ============================================================\n",
   "from sklearn.metrics import cohen_kappa_score\n",
   "\n",
   "print(\"=\" * 70)\n",
   "print(\"QUALITATIVE ANALYSIS\")\n",
   "print(\"=\" * 70)\n",
   "\n",
   "# --- LLM-Based Coding Function ---\n",
   "def llm_qualitative_coding(responses, project_name, model=None):\n",
   "    \"\"\"\n",
   "    Use Gemini to perform open coding on qualitative responses.\n",
   "    Returns coded data with themes.\n",
   "    \"\"\"\n",
   "    if model is None:\n",
   "        print(f\"  ℹ LLM not available. Using pre-coded data for {project_name}.\")\n",
   "        return None\n",
   "    \n",
   "    prompt = f\"\"\"You are a qualitative researcher performing thematic analysis.\n",
   "    \n",
   "Analyze these open-ended responses from a usability study of {project_name}.\n",
   "For each response, assign:\n",
   "1. A descriptive code (2-3 words)\n",
   "2. A broader theme category\n",
   "\n",
   "Responses:\n",
   "{chr(10).join(f'- {r}' for r in responses[:20])}\n",
   "\n",
   "Return as JSON array: [{{\"response\": \"...\", \"code\": \"...\", \"theme\": \"...\"}}]\"\"\"\n",
   "    \n",
   "    try:\n",
   "        response = model.generate_content(prompt)\n",
   "        return json.loads(response.text)\n",
   "    except Exception as e:\n",
   "        print(f\"  ✗ LLM coding failed: {e}\")\n",
   "        return None\n",
   "\n",
   "# --- Load Pre-coded Data ---\n",
   "print(\"\\n─── PROJECT 1: Qualitative Results ───\")\n",
   "p1_coded_path = P1_DIR / 'posttest' / 'coded_qualitative_data.csv'\n",
   "try:\n",
   "    p1_coded = pd.read_csv(p1_coded_path)\n",
   "    print(f\"✓ Loaded pre-coded data: {len(p1_coded)} coded segments\")\n",
   "    \n",
   "    # Theme distribution\n",
   "    print(f\"\\nTheme Distribution:\")\n",
   "    theme_counts = p1_coded['theme'].value_counts()\n",
   "    for theme, count in theme_counts.items():\n",
   "        pct = count / len(p1_coded) * 100\n",
   "        print(f\"  {theme}: {count} ({pct:.1f}%)\")\n",
   "    \n",
   "    # Code distribution (top 10)\n",
   "    print(f\"\\nTop 10 Codes:\")\n",
   "    code_counts = p1_coded['code'].value_counts().head(10)\n",
   "    for code, count in code_counts.items():\n",
   "        print(f\"  {code}: {count}\")\n",
   "    \n",
   "except FileNotFoundError:\n",
   "    print(\"✗ Pre-coded data not found.\")\n",
   "    p1_coded = None\n",
   "\n",
   "# --- Inter-Rater Reliability ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"INTER-RATER RELIABILITY\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "if p1_coded is not None and 'coder' in p1_coded.columns:\n",
   "    # Get the two coders' assignments\n",
   "    coder1_data = p1_coded[p1_coded['coder'] == 'LLM_coder_1']\n",
   "    coder2_data = p1_coded[p1_coded['coder'] == 'LLM_coder_2']\n",
   "    \n",
   "    # Match by participant_id and response_type\n",
   "    merged = coder1_data.merge(coder2_data, on=['participant_id', 'response_type'], \n",
   "                                suffixes=('_c1', '_c2'), how='inner')\n",
   "    \n",
   "    if len(merged) > 0:\n",
   "        # Cohen's Kappa for theme-level agreement\n",
   "        kappa_theme = cohen_kappa_score(merged['theme_c1'], merged['theme_c2'])\n",
   "        print(f\"\\nProject 1 — Inter-Rater Reliability:\")\n",
   "        print(f\"  Cohen's Kappa (themes): κ = {kappa_theme:.3f}\")\n",
   "        \n",
   "        quality = (\"Almost Perfect\" if kappa_theme > 0.8 else \"Substantial\" if kappa_theme > 0.6 \n",
   "                   else \"Moderate\" if kappa_theme > 0.4 else \"Fair\" if kappa_theme > 0.2 else \"Slight\")\n",
   "        print(f\"  Agreement Level: {quality}\")\n",
   "        \n",
   "        # Code-level agreement\n",
   "        kappa_code = cohen_kappa_score(merged['code_c1'], merged['code_c2'])\n",
   "        print(f\"  Cohen's Kappa (codes): κ = {kappa_code:.3f}\")\n",
   "        \n",
   "        # Percentage agreement\n",
   "        theme_agree = (merged['theme_c1'] == merged['theme_c2']).mean() * 100\n",
   "        code_agree = (merged['code_c1'] == merged['code_c2']).mean() * 100\n",
   "        print(f\"  Percentage agreement (themes): {theme_agree:.1f}%\")\n",
   "        print(f\"  Percentage agreement (codes): {code_agree:.1f}%\")\n",
   "        \n",
   "        # Krippendorff's Alpha (simplified computation)\n",
   "        try:\n",
   "            import krippendorff\n",
   "            # Prepare data for krippendorff\n",
   "            theme_labels = list(set(merged['theme_c1'].tolist() + merged['theme_c2'].tolist()))\n",
   "            c1_numeric = [theme_labels.index(t) for t in merged['theme_c1']]\n",
   "            c2_numeric = [theme_labels.index(t) for t in merged['theme_c2']]\n",
   "            reliability_data = [c1_numeric, c2_numeric]\n",
   "            kalpha = krippendorff.alpha(reliability_data=reliability_data, level_of_measurement='nominal')\n",
   "            print(f\"  Krippendorff's Alpha: α = {kalpha:.3f}\")\n",
   "        except ImportError:\n",
   "            print(\"  ℹ Krippendorff package not available. Install with: pip install krippendorff\")\n",
   "    else:\n",
   "        print(\"  ✗ Could not match coders for reliability computation.\")\n",
   "\n",
   "# --- Project 2 Qualitative ---\n",
   "print(f\"\\n─── PROJECT 2: Qualitative Results ───\")\n",
   "p2_coded_path = P2_DIR / 'posttest' / 'coded_qualitative_data.csv'\n",
   "try:\n",
   "    p2_coded = pd.read_csv(p2_coded_path)\n",
   "    print(f\"✓ Loaded pre-coded data: {len(p2_coded)} coded segments\")\n",
   "    \n",
   "    theme_counts = p2_coded['theme'].value_counts()\n",
   "    print(f\"\\nTheme Distribution:\")\n",
   "    for theme, count in theme_counts.items():\n",
   "        pct = count / len(p2_coded) * 100\n",
   "        print(f\"  {theme}: {count} ({pct:.1f}%)\")\n",
   "    \n",
   "    # IRR for P2\n",
   "    coder1_p2 = p2_coded[p2_coded['coder'] == 'LLM_coder_1']\n",
   "    coder2_p2 = p2_coded[p2_coded['coder'] == 'LLM_coder_2']\n",
   "    merged_p2 = coder1_p2.merge(coder2_p2, on=['participant_id', 'response_type'],\n",
   "                                  suffixes=('_c1', '_c2'), how='inner')\n",
   "    \n",
   "    if len(merged_p2) > 0:\n",
   "        kappa_p2 = cohen_kappa_score(merged_p2['theme_c1'], merged_p2['theme_c2'])\n",
   "        print(f\"\\nProject 2 — Inter-Rater Reliability:\")\n",
   "        print(f\"  Cohen's Kappa (themes): κ = {kappa_p2:.3f}\")\n",
   "        code_agree_p2 = (merged_p2['code_c1'] == merged_p2['code_c2']).mean() * 100\n",
   "        print(f\"  Percentage agreement (codes): {code_agree_p2:.1f}%\")\n",
   "        \n",
   "except FileNotFoundError:\n",
   "    print(\"✗ Pre-coded data not found.\")\n",
   "    p2_coded = None\n",
   "\n",
   "# --- Attempt LLM Coding (Gemini) ---\n",
   "print(f\"\\n\" + \"─\" * 70)\n",
   "print(\"LLM-BASED CODING DEMONSTRATION\")\n",
   "print(\"─\" * 70)\n",
   "\n",
   "if GEMINI_AVAILABLE and p1_posttest is not None and 'open_positive' in p1_posttest.columns:\n",
   "    print(\"Attempting LLM-based coding with Gemini...\")\n",
   "    sample_responses = p1_posttest['open_positive'].dropna().head(5).tolist()\n",
   "    \n",
   "    try:\n",
   "        coding_prompt = f\"\"\"Perform thematic coding on these user feedback responses from a usability study of an AI-generated videogame.\n",
   "\n",
   "For each response, provide:\n",
   "1. A descriptive code (2-3 words, lowercase with underscores)\n",
   "2. A broader theme\n",
   "\n",
   "Responses:\n",
   "{chr(10).join(f'{i+1}. \"{r}\"' for i, r in enumerate(sample_responses))}\n",
   "\n",
   "Return ONLY a JSON array like: [{{\"response_num\": 1, \"code\": \"example_code\", \"theme\": \"Example Theme\"}}]\"\"\"\n",
   "\n",
   "        response = gemini_model.generate_content(coding_prompt)\n",
   "        print(f\"✓ LLM coding result (sample of 5 responses):\")\n",
   "        # Try to parse and display\n",
   "        try:\n",
   "            result_text = response.text\n",
   "            if '```json' in result_text:\n",
   "                result_text = result_text.split('```json')[1].split('```')[0]\n",
   "            elif '```' in result_text:\n",
   "                result_text = result_text.split('```')[1].split('```')[0]\n",
   "            coded_results = json.loads(result_text)\n",
   "            for item in coded_results:\n",
   "                print(f\"  Response {item.get('response_num', '?')}: code='{item.get('code', 'N/A')}', theme='{item.get('theme', 'N/A')}'\")\n",
   "        except:\n",
   "            print(f\"  Raw response: {response.text[:500]}\")\n",
   "    except Exception as e:\n",
   "        print(f\"✗ LLM coding failed: {e}\")\n",
   "else:\n",
   "    print(\"LLM coding skipped (Gemini not available or no data).\")\n",
   "    print(\"In a live session, this would use Gemini to code open-ended responses.\")\n",
   "    print(\"The pre-coded data (loaded above) serves as the fallback.\")"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# SECTION 6.4: MIXED METHODS INTEGRATION — JOINT DISPLAY\n",
   "# ============================================================\n",
   "print(\"=\" * 70)\n",
   "print(\"MIXED METHODS INTEGRATION: JOINT DISPLAY TABLES\")\n",
   "print(\"=\" * 70)\n",
   "\n",
   "print(\"\"\"\n",
   "┌─────────────────────────────────────────────────────────────────────────────┐\n",
   "│                    PROJECT 1: GEMINI QUEST — JOINT DISPLAY                 │\n",
   "├──────────────────────┬──────────────────────┬───────────────────────────────┤\n",
   "│ Quantitative Finding │ Qualitative Theme    │ Integration                   │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ SUS Score: ~72       │ \"User Experience\"    │ Convergent: Above-average     │\n",
   "│ (Above average)      │ theme shows mixed    │ usability confirmed by        │\n",
   "│                      │ UI navigation issues │ specific navigation concerns  │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ Narrative Quality:   │ \"Content Quality\"    │ Convergent: High ratings      │\n",
   "│ High ratings (>5/7)  │ theme dominated by   │ align with positive narrative  │\n",
   "│                      │ positive story codes │ feedback                      │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ AI Perception:       │ \"AI Perception\"      │ Complementary: Quantitative   │\n",
   "│ Moderate (~4/7)      │ shows divided views  │ shows central tendency;       │\n",
   "│                      │ on AI authenticity   │ qualitative reveals nuances   │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ Engagement UES:      │ \"Engagement\" theme   │ Convergent: Behavioral data   │\n",
   "│ Above midpoint       │ mentions immersion,  │ (session time) supports       │\n",
   "│                      │ replayability        │ self-reported engagement      │\n",
   "└──────────────────────┴──────────────────────┴───────────────────────────────┘\n",
   "\"\"\")\n",
   "\n",
   "print(\"\"\"\n",
   "┌─────────────────────────────────────────────────────────────────────────────┐\n",
   "│                    PROJECT 2: STUDYBUDDY — JOINT DISPLAY                   │\n",
   "├──────────────────────┬──────────────────────┬───────────────────────────────┤\n",
   "│ Quantitative Finding │ Qualitative Theme    │ Integration                   │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ SUS Score: ~68       │ \"Usability\" theme    │ Convergent: Borderline SUS    │\n",
   "│ (Borderline OK/Good) │ mentions learning    │ explained by UI complexity    │\n",
   "│                      │ curve issues         │ concerns in feedback          │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ Trust: Moderate      │ \"Trust & Transparency\"│ Complementary: Moderate      │\n",
   "│ (~4/7)               │ requests for more    │ trust scores explained by     │\n",
   "│                      │ explanation of AI    │ desire for transparency       │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ Privacy: High concern│ \"Privacy\" theme      │ Convergent: Both confirm      │\n",
   "│ (~5/7)               │ data control and     │ privacy as a major concern    │\n",
   "│                      │ transparency demands │ for student users             │\n",
   "├──────────────────────┼──────────────────────┼───────────────────────────────┤\n",
   "│ Usefulness: Positive │ \"Engagement\" theme   │ Convergent: Students find     │\n",
   "│ (~5/7)               │ mentions motivation, │ tool useful despite trust     │\n",
   "│                      │ goal-setting value   │ reservations                  │\n",
   "└──────────────────────┴──────────────────────┴───────────────────────────────┘\n",
   "\"\"\")\n",
   "\n",
   "# Summary statistics table for the paper\n",
   "print(\"\\n\" + \"=\" * 70)\n",
   "print(\"SUMMARY TABLE FOR PAPER (Both Projects)\")\n",
   "print(\"=\" * 70)\n",
   "\n",
   "summary_data = []\n",
   "if p1_posttest is not None:\n",
   "    summary_data.append({\n",
   "        'Project': 'Gemini Quest',\n",
   "        'N': len(p1_posttest),\n",
   "        'SUS (M±SD)': f\"{p1_posttest['sus_score'].mean():.1f}±{p1_posttest['sus_score'].std():.1f}\",\n",
   "    })\n",
   "    if 'nq_mean' in p1_posttest.columns:\n",
   "        summary_data[-1]['Primary Scale (M±SD)'] = f\"NQ: {p1_posttest['nq_mean'].mean():.2f}±{p1_posttest['nq_mean'].std():.2f}\"\n",
   "\n",
   "if p2_posttest is not None:\n",
   "    summary_data.append({\n",
   "        'Project': 'StudyBuddy',\n",
   "        'N': len(p2_posttest),\n",
   "        'SUS (M±SD)': f\"{p2_posttest['sus_score'].mean():.1f}±{p2_posttest['sus_score'].std():.1f}\",\n",
   "    })\n",
   "    if 'trust_mean' in p2_posttest.columns:\n",
   "        summary_data[-1]['Primary Scale (M±SD)'] = f\"Trust: {p2_posttest['trust_mean'].mean():.2f}±{p2_posttest['trust_mean'].std():.2f}\"\n",
   "\n",
   "if summary_data:\n",
   "    summary_df = pd.DataFrame(summary_data)\n",
   "    print(summary_df.to_string(index=False))"
  ],
  "outputs": [],
  "execution_count": null
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "---\n",
   "<a id=\"section-7\"></a>\n",
   "# Section 7: Research Report\n",
   "\n",
   "## Writing for ACM CHI\n",
   "\n",
   "The following section presents a draft research paper structured for submission to ACM CHI. The paper follows the standard SIGCHI format:\n",
   "\n",
   "1. **Abstract** — Summary of problem, method, findings\n",
   "2. **Introduction** — Motivation, research gap, contributions\n",
   "3. **Related Work** — Prior literature positioning\n",
   "4. **Methodology** — Study design, participants, instruments, procedure\n",
   "5. **Results** — Quantitative findings, qualitative themes, integration\n",
   "6. **Discussion** — Interpretation, implications, design guidelines\n",
   "7. **Limitations** — Methodological constraints and LLM automation concerns\n",
   "8. **Conclusion** — Summary and future work\n",
   "\n",
   "> **Note:** This draft uses the dummy data analyzed in previous sections. In a real submission, replace with actual participant data and update all statistics accordingly."
  ],
  "outputs": []
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
   "## Research Paper Draft\n",
   "\n",
   "---\n",
   "\n",
   "# Human-Centered Design of AI-Generated Interactive Experiences: A Mixed-Methods Evaluation of Multimodal LLM and AutoML Prototypes\n",
   "\n",
   "## Abstract\n",
   "\n",
   "We present a comprehensive human-centered design study of two AI-powered prototypes: (1) **Gemini Quest**, an interactive narrative videogame generated entirely through multimodal large language models (Gemini), and (2) **StudyBuddy**, an adaptive study companion powered by AutoML (AutoGluon). Through a sequential mixed-methods approach involving requirements surveys (N=120 per project) and usability evaluations (N=40 per project), we investigate user perceptions of AI-generated content quality, trust in AI recommendations, and the design factors that most influence user experience. Our findings reveal that users rate AI-generated narratives positively (M=5.2/7) but express moderate reservations about AI authenticity (M=4.1/7), while study companion users demonstrate a tension between perceived usefulness (M=5.0/7) and privacy concerns (M=4.8/7). We contribute design guidelines for human-centered AI systems, empirical evidence on user perception of multimodal AI content, and a methodological framework for rapid prototyping with LLM-generated code. We discuss the implications and limitations of LLM automation throughout the research pipeline.\n",
   "\n",
   "**Keywords:** Human-Centered AI, Generative AI, AutoML, Mixed Methods, Usability, Interactive Narrative, Educational Technology\n",
   "\n",
   "---\n",
   "\n",
   "## 1. Introduction\n",
   "\n",
   "The rapid advancement of generative AI models has created unprecedented opportunities for creating interactive experiences. Large Language Models (LLMs) can now generate text, images, music, and code, while AutoML frameworks enable non-experts to build predictive models. However, the integration of these capabilities into user-facing systems raises fundamental questions about user experience, trust, and acceptance.\n",
   "\n",
   "Two domains exemplify the challenges and opportunities of Human-Centered AI (HCAI):\n",
   "\n",
   "**Interactive entertainment** presents a unique case where the entirety of the user experience—narrative, visuals, audio, and mechanics—can be AI-generated. Prior work has explored individual modalities (e.g., AI storytelling [Riedl & Bulitko, 2013], AI art [Epstein et al., 2023]), but the holistic user experience of fully AI-generated games remains understudied.\n",
   "\n",
   "**Educational technology** offers a high-stakes application where AI predictions directly influence student behavior. AutoML tools like AutoGluon [Erickson et al., 2020] democratize ML, but presenting predictions to students requires careful consideration of trust, transparency, and privacy [Holstein et al., 2019].\n",
   "\n",
   "### Research Contributions\n",
   "1. A user-centered design framework for multimodal AI game generation informed by requirements surveys\n",
   "2. Empirical evaluation of an AutoML-powered study companion with focus on trust and privacy\n",
   "3. Design guidelines for both domains derived from mixed-methods analysis\n",
   "4. Methodological insights on using LLMs in the research pipeline itself\n",
   "\n",
   "---\n",
   "\n",
   "## 2. Related Work\n",
   "\n",
   "### 2.1 AI-Generated Interactive Narratives\n",
   "Interactive narrative systems have evolved from rule-based engines to neural approaches. Kreminski & Wardrip-Fruin (2019) explored generative games as expressive AI systems. Recent work by Lanzi & Loiacono (2023) demonstrated using ChatGPT for game design, while Mirowski et al. (2023) showed LLMs can generate coherent dramatic narratives. Our work extends this by integrating multiple generative modalities and evaluating the complete user experience through validated instruments.\n",
   "\n",
   "### 2.2 AI in Education\n",
   "Intelligent tutoring systems have a long history in HCI [VanLehn, 2011]. Recent developments in AutoML [Erickson et al., 2020] enable rapid development of predictive models for educational contexts. Khosravi et al. (2022) highlighted the need for explainable AI in education. Our work specifically examines how human-centered design principles can improve student trust and acceptance of AI-driven study recommendations.\n",
   "\n",
   "### 2.3 Human-Centered AI Design\n",
   "Amershi et al. (2019) proposed 18 guidelines for human-AI interaction. Shneiderman (2022) advocated for human-centered approaches that balance automation with human control. Our study applies these principles in practice, measuring their impact on user experience across two distinct AI applications.\n",
   "\n",
   "---\n",
   "\n",
   "## 3. Methodology\n",
   "\n",
   "### 3.1 Study Design\n",
   "We employed a **mixed-methods sequential explanatory design** [Creswell & Clark, 2017]:\n",
   "- **Phase 1 (Requirements):** Online survey (N=120 per project) to gather user preferences and inform prototype design\n",
   "- **Phase 2 (Evaluation):** Lab-based usability study (N=40 per project) with post-test surveys and behavioral telemetry\n",
   "\n",
   "### 3.2 Participants\n",
   "\n",
   "**Requirements Survey:**\n",
   "- Project 1: 120 participants (Age: M=25.3, SD=7.2; 52% Male, 40% Female, 8% Non-binary/Other)\n",
   "- Project 2: 120 participants (Age: M=22.1, SD=4.5; 48% Male, 44% Female, 8% Non-binary/Other)\n",
   "- Recruited through university participant pools and social media\n",
   "\n",
   "**Usability Evaluation:**\n",
   "- 40 participants per project\n",
   "- Sample size determined by a priori power analysis (Cohen’s d=0.5, α=0.05, power=0.80)\n",
   "- Compensated with $15 USD gift cards\n",
   "\n",
   "### 3.3 Instruments\n",
   "- System Usability Scale (SUS; Brooke, 1996)\n",
   "- User Engagement Scale Short Form (UES-SF; O’Brien et al., 2018) [Project 1]\n",
   "- Trust in AI scale (adapted from Madsen & Gregor, 2000) [Project 2]\n",
   "- Technology Acceptance Model scales (Davis, 1989) [Project 2]\n",
   "- Custom scales for narrative quality, AI perception, immersion (Project 1) and accuracy perception, privacy concern (Project 2)\n",
   "- Behavioral telemetry (session duration, clicks, feature usage, navigation patterns)\n",
   "\n",
   "### 3.4 Procedure\n",
   "1. Informed consent and demographics collection (5 min)\n",
   "2. Brief prototype orientation (2 min)\n",
   "3. Free exploration of the prototype (15-30 min)\n",
   "4. Post-test survey completion (10-15 min)\n",
   "5. Optional debrief (5 min)\n",
   "\n",
   "### 3.5 Analysis Approach\n",
   "- **Quantitative:** Descriptive statistics, reliability analysis (Cronbach’s α), normality testing (Shapiro-Wilk), hypothesis testing (t-tests, ANOVA, correlations), effect sizes (Cohen’s d, η²)\n",
   "- **Qualitative:** Reflexive thematic analysis [Braun & Clarke, 2006], with LLM-assisted initial coding and human verification\n",
   "- **Integration:** Joint display tables, convergence/complementarity analysis\n",
   "\n",
   "---\n",
   "\n",
   "## 4. Results\n",
   "\n",
   "### 4.1 Project 1: Gemini Quest\n",
   "\n",
   "**Usability:** The prototype achieved a mean SUS score of 72.1 (SD=12.3), exceeding the industry benchmark of 68 and falling in the “Good” category [Bangor et al., 2009].\n",
   "\n",
   "**Engagement:** UES-SF subscale scores indicated above-average engagement across all dimensions, with Aesthetic Appeal showing the highest ratings (M=3.8/5, SD=0.6).\n",
   "\n",
   "**Narrative Quality:** Users rated the AI-generated narrative positively (M=5.2/7, SD=1.1), with particularly high scores for story engagement (M=5.4/7) and pacing (M=5.1/7).\n",
   "\n",
   "**AI Perception:** Moderate ratings (M=4.1/7, SD=1.3) suggest users recognized both the potential and limitations of AI-generated content. Item-level analysis revealed that “distinguishability from human content” received the lowest scores (M=3.5/7).\n",
   "\n",
   "**Hypothesis Testing:**\n",
   "- H1 (Preferences → SUS): No significant difference between preference-integrated and generic conditions (t(38)=0.82, p=.418, d=0.26). This may be due to the small sample size and condition assignment.\n",
   "- H2 (Narrative × Engagement): Significant positive correlation (r=0.45, p=.003), supporting the hypothesis.\n",
   "- H3 (AI Awareness × Immersion): Trend-level difference (t(38)=-1.41, p=.166, d=0.45).\n",
   "\n",
   "**Qualitative Themes:**\n",
   "Five themes emerged from thematic analysis:\n",
   "1. **Content Quality** (34%): Predominantly positive feedback on story coherence and creativity\n",
   "2. **User Experience** (24%): Mixed feedback on navigation and pacing\n",
   "3. **AI Perception** (18%): Divided views on AI authenticity\n",
   "4. **Engagement** (14%): Reports of immersion and desire for replayability\n",
   "5. **Technical Performance** (10%): Occasional glitches noted\n",
   "\n",
   "### 4.2 Project 2: StudyBuddy\n",
   "\n",
   "**Usability:** Mean SUS score of 68.2 (SD=14.1), at the borderline between “OK” and “Good.”\n",
   "\n",
   "**Trust:** Moderate trust ratings (M=4.2/7, SD=1.2) suggest cautious acceptance of AI predictions.\n",
   "\n",
   "**Usefulness:** Perceived usefulness was positive (M=5.0/7, SD=1.0), indicating students recognized the tool’s potential value.\n",
   "\n",
   "**Privacy:** Elevated privacy concerns (M=4.8/7, SD=1.3) represent the most notable finding, significantly correlating with lower trust (r=-0.38, p=.015).\n",
   "\n",
   "**Hypothesis Testing:**\n",
   "- H1 (Explanation → Trust): Non-significant main effect of explanation level on trust (F(2,37)=1.24, p=.301, η²=.063).\n",
   "- H2 (Usefulness × Usage): Moderate positive correlation (ρ=0.31, p=.049).\n",
   "- H3 (Privacy × Trust): Significant negative correlation (r=-0.38, p=.015), confirming the privacy-trust tension.\n",
   "\n",
   "**Qualitative Themes:**\n",
   "Five themes emerged:\n",
   "1. **Trust & Transparency** (28%): Desire for explanation of how predictions are made\n",
   "2. **Usability** (24%): Learning curve concerns, feature discoverability\n",
   "3. **Privacy** (20%): Data control and institutional use concerns\n",
   "4. **AI Accuracy** (16%): Questions about prediction reliability\n",
   "5. **Engagement & Motivation** (12%): Positive reports of goal-setting and motivation\n",
   "\n",
   "---\n",
   "\n",
   "## 5. Discussion\n",
   "\n",
   "### 5.1 AI-Generated Content Quality\n",
   "Our findings suggest that users are generally receptive to AI-generated interactive narratives, with narrative quality ratings exceeding the scale midpoint. However, the moderate AI perception scores indicate that full acceptance remains elusive. Users are particularly sensitive to authenticity—whether AI content “feels” human-made. This aligns with recent work on the “uncanny valley” of AI-generated text (Jakesch et al., 2023).\n",
   "\n",
   "**Design Guideline 1:** *Prioritize narrative coherence over realism.* Users valued engaging stories over perfect mimicry of human writing.\n",
   "\n",
   "### 5.2 Trust-Privacy Tension in Educational AI\n",
   "The significant negative correlation between privacy concerns and trust highlights a fundamental tension in AI-powered educational tools. Students recognize the potential utility of performance predictions but remain wary of data collection. This finding echoes Holstein et al.’s (2019) call for fairness-aware ML in education.\n",
   "\n",
   "**Design Guideline 2:** *Provide granular data controls and transparent data usage policies.* Students need to understand and control what data the system uses.\n",
   "\n",
   "**Design Guideline 3:** *Show prediction confidence and limitations.* Moderate trust levels suggest that overconfident predictions may backfire.\n",
   "\n",
   "### 5.3 The Role of User Requirements in AI Design\n",
   "Our requirements surveys informed specific design decisions (genre, art style, dashboard complexity) that shaped the prototypes. While H1 in Project 1 did not reach significance, the trend suggests that user preference integration may improve the experience, warranting investigation with larger samples.\n",
   "\n",
   "**Design Guideline 4:** *Integrate user preference surveys into the AI generation pipeline.* Even when effects are small, alignment with user expectations demonstrates respect for user agency.\n",
   "\n",
   "### 5.4 Implications for HCI Research Methodology\n",
   "This study demonstrates a complete human-centered design pipeline with significant LLM automation. While this accelerates prototyping and analysis, it raises important questions about scientific validity (see Limitations).\n",
   "\n",
   "---\n",
   "\n",
   "## 6. Limitations\n",
   "\n",
   "### 6.1 LLM Automation in the Research Pipeline\n",
   "\n",
   "This study used LLMs (Google Gemini) at multiple stages of the research pipeline, which introduces several methodological concerns:\n",
   "\n",
   "#### Prototype Generation\n",
   "The prototypes were generated using Gemini’s code generation capabilities. While functional, LLM-generated code may contain subtle biases in UI design, interaction patterns, or content that could influence user responses. **Mitigation:** Pre-generated prototypes were reviewed and tested by the research team before deployment.\n",
   "\n",
   "#### Qualitative Coding\n",
   "LLM-based qualitative coding raises the most significant validity concern. Two issues are paramount:\n",
   "1. **Independence:** LLM “coders” are not truly independent since they share the same underlying model. High inter-rater reliability between LLM coders may reflect model consistency rather than genuine interpretive agreement.\n",
   "2. **Interpretive Depth:** LLMs may miss context-dependent meanings, cultural nuances, and implicit sentiments that human coders would capture [Tai et al., 2024].\n",
   "\n",
   "**Mitigation strategies applied:**\n",
   "- Used multiple prompts to simulate coding independence\n",
   "- Computed inter-rater reliability (Cohen’s κ, Krippendorff’s α)\n",
   "- Flagged LLM coding as preliminary analysis in reporting\n",
   "- **Recommended for full validity:** Human verification of ≥20% of coded segments\n",
   "\n",
   "#### Data Generation\n",
   "For this workshop demonstration, dummy data was used in place of actual participant responses. All statistical results should be interpreted as illustrative of the analysis pipeline, not as empirical findings.\n",
   "\n",
   "### 6.2 Sample Size and Generalizability\n",
   "- The evaluation sample (N=40 per project) provides adequate power for medium effects but may miss smaller effects\n",
   "- University student sample limits generalizability to broader populations\n",
   "- Single-session evaluation may not capture longitudinal usage patterns\n",
   "\n",
   "### 6.3 Ecological Validity\n",
   "- Lab-based evaluation may not reflect naturalistic use\n",
   "- Fixed game content limits assessment of generative variety\n",
   "- StudyBuddy predictions are simulated, not based on actual student data\n",
   "\n",
   "### 6.4 Instrument Limitations\n",
   "- Custom scales (narrative quality, AI perception) require further validation\n",
   "- Self-reported measures may be subject to social desirability bias\n",
   "- Telemetry provides behavioral data but not motivational context\n",
   "\n",
   "### 6.5 Scientific Validity of LLM-Automated Research\n",
   "The use of LLMs throughout the research pipeline (from code generation to data analysis) represents a methodological experiment in itself. While LLM automation can democratize and accelerate research, several concerns must be addressed for results to meet the scientific standards of venues like ACM CHI:\n",
   "\n",
   "1. **Reproducibility:** LLM outputs are non-deterministic. Temperature settings and model versions should be reported.\n",
   "2. **Bias propagation:** LLM biases in code generation may create prototypes that privilege certain user groups.\n",
   "3. **Transparency:** All LLM-assisted steps must be clearly disclosed in publications.\n",
   "4. **Human oversight:** A human-in-the-loop approach is essential—LLMs should assist, not replace, researcher judgment.\n",
   "5. **Validation:** Results from LLM-automated analyses should be validated against traditional methods on a subset of data.\n",
   "\n",
   "> *“The goal is not to eliminate human involvement but to augment human capabilities while maintaining scientific rigor.”*\n",
   "\n",
   "---\n",
   "\n",
   "## 7. Conclusion\n",
   "\n",
   "This study demonstrates a complete human-centered design pipeline for two AI-powered prototypes, from user requirements gathering through usability evaluation and mixed-methods analysis. Our findings suggest that users are receptive to AI-generated interactive content but require transparency, control, and quality assurance. The trust-privacy tension in educational AI tools highlights the need for thoughtful design that prioritizes student agency.\n",
   "\n",
   "We contribute four design guidelines, empirical evidence on user perception of multimodal AI, and a methodological framework for LLM-assisted rapid prototyping. Future work should validate these findings with larger and more diverse samples, conduct longitudinal evaluations, and further investigate the boundaries of acceptable LLM automation in HCI research.\n",
   "\n",
   "---\n",
   "\n",
   "## References\n",
   "\n",
   "- Amershi, S., et al. (2019). Guidelines for Human-AI Interaction. *Proc. ACM CHI*.\n",
   "- Bangor, A., Kortum, P., & Miller, J. (2009). Determining what individual SUS scores mean. *J. Usability Studies*, 4(3).\n",
   "- Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. *Qualitative Research in Psychology*, 3(2).\n",
   "- Brooke, J. (1996). SUS: A ‘quick and dirty’ usability scale. *Usability Evaluation in Industry*.\n",
   "- Creswell, J. W., & Clark, V. L. P. (2017). *Designing and Conducting Mixed Methods Research*. Sage.\n",
   "- Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance. *MIS Quarterly*, 13(3).\n",
   "- Epstein, Z., et al. (2023). Art and the science of generative AI. *Science*, 380(6650).\n",
   "- Erickson, N., et al. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data.\n",
   "- Holstein, K., et al. (2019). Improving fairness in ML systems. *Proc. ACM CHI*.\n",
   "- Jakesch, M., et al. (2023). Human heuristics for AI-generated language are flawed. *PNAS*.\n",
   "- Khosravi, H., et al. (2022). Explainable AI in education. *Computers and Education: AI*.\n",
   "- Kreminski, M., & Wardrip-Fruin, N. (2019). Generative games as expressive AI. *Proc. FDG*.\n",
   "- Lanzi, P. L., & Loiacono, D. (2023). ChatGPT for Online Interactive Collaborative Game Design. *Proc. GECCO*.\n",
   "- Lazar, J., Feng, J. H., & Hochheiser, H. (2017). *Research Methods in HCI*. Morgan Kaufmann.\n",
   "- Madsen, M., & Gregor, S. (2000). Measuring human-computer trust. *Proc. ACIS*.\n",
   "- O’Brien, H. L., et al. (2018). A practical approach to measuring user engagement. *IJHCS*, 112.\n",
   "- Riedl, M. O., & Bulitko, V. (2013). Interactive narrative: An intelligent systems approach. *AI Magazine*, 34(1).\n",
   "- Shneiderman, B. (2022). *Human-Centered AI*. Oxford University Press.\n",
   "- Tai, R. H., et al. (2024). LLMs to Aid Analysis of Textual Data. *Int. J. Qualitative Methods*.\n",
   "- VanLehn, K. (2011). The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems. *Educational Psychologist*, 46(4).\n",
   "- Xiao, Z., et al. (2023). Supporting Qualitative Analysis with LLMs. *Proc. ACM CHI*."
  ],
  "outputs": []
 },
 {
  "cell_type": "code",
  "metadata": {},
  "source": [
   "# ============================================================\n",
   "# SECTION 7: Save Research Paper\n",
   "# ============================================================\n",
   "# The research paper content above is saved for reference\n",
   "paper_path = DELIVERABLES / 'report' / 'research_paper_draft.md'\n",
   "\n",
   "# The paper is written in the markdown cell above.\n",
   "# Here we save a summary of all key statistics for easy reference.\n",
   "\n",
   "print(\"=\" * 70)\n",
   "print(\"KEY STATISTICS SUMMARY FOR PAPER\")\n",
   "print(\"=\" * 70)\n",
   "\n",
   "stats_summary = []\n",
   "\n",
   "if p1_posttest is not None:\n",
   "    stats_summary.append(\"PROJECT 1: GEMINI QUEST\")\n",
   "    stats_summary.append(f\"  N = {len(p1_posttest)}\")\n",
   "    stats_summary.append(f\"  SUS: M={p1_posttest['sus_score'].mean():.1f}, SD={p1_posttest['sus_score'].std():.1f}\")\n",
   "    if 'nq_mean' in p1_posttest.columns:\n",
   "        stats_summary.append(f\"  Narrative Quality: M={p1_posttest['nq_mean'].mean():.2f}, SD={p1_posttest['nq_mean'].std():.2f}\")\n",
   "    if 'ai_mean' in p1_posttest.columns:\n",
   "        stats_summary.append(f\"  AI Perception: M={p1_posttest['ai_mean'].mean():.2f}, SD={p1_posttest['ai_mean'].std():.2f}\")\n",
   "    if 'immersion_mean' in p1_posttest.columns:\n",
   "        stats_summary.append(f\"  Immersion: M={p1_posttest['immersion_mean'].mean():.2f}, SD={p1_posttest['immersion_mean'].std():.2f}\")\n",
   "    stats_summary.append(\"\")\n",
   "\n",
   "if p2_posttest is not None:\n",
   "    stats_summary.append(\"PROJECT 2: STUDYBUDDY\")\n",
   "    stats_summary.append(f\"  N = {len(p2_posttest)}\")\n",
   "    stats_summary.append(f\"  SUS: M={p2_posttest['sus_score'].mean():.1f}, SD={p2_posttest['sus_score'].std():.1f}\")\n",
   "    if 'trust_mean' in p2_posttest.columns:\n",
   "        stats_summary.append(f\"  Trust: M={p2_posttest['trust_mean'].mean():.2f}, SD={p2_posttest['trust_mean'].std():.2f}\")\n",
   "    if 'usefulness_mean' in p2_posttest.columns:\n",
   "        stats_summary.append(f\"  Usefulness: M={p2_posttest['usefulness_mean'].mean():.2f}, SD={p2_posttest['usefulness_mean'].std():.2f}\")\n",
   "    if 'privacy_mean' in p2_posttest.columns:\n",
   "        stats_summary.append(f\"  Privacy Concern: M={p2_posttest['privacy_mean'].mean():.2f}, SD={p2_posttest['privacy_mean'].std():.2f}\")\n",
   "\n",
   "for line in stats_summary:\n",
   "    print(line)\n",
   "\n",
   "# Save the paper\n",
   "print(f\"\\n✓ Research paper draft is in the markdown cell above\")\n",
   "print(f\"  Copy to your preferred word processor for formatting\")\n",
   "print(f\"  Apply ACM CHI template: https://chi2025.acm.org/submission-guides/\")\n",
   "\n",
   "print(f\"\\n{'='*70}\")\n",
   "print(\"WORKSHOP COMPLETE\")\n",
   "print(\"=\"*70)\n",
   "print(\"\"\"\n",
   "Congratulations! You have completed the full end-to-end pipeline:\n",
   "\n",
   "  1. ✓ Project Definition — Research questions, hypotheses, power analysis\n",
   "  2. ✓ User Requirements — Survey design, data collection/generation\n",
   "  3. ✓ Integrate Feedback — Design specs, AI-generated prototypes  \n",
   "  4. ✓ Deploy Prototype — Static web apps with telemetry\n",
   "  5. ✓ User Evaluation — Post-test surveys, interaction logs\n",
   "  6. ✓ Analyses — Quantitative stats, qualitative coding, mixed methods\n",
   "  7. ✓ Report — Conference paper draft with results\n",
   "\n",
   "Next Steps:\n",
   "  • Replace dummy data with real participant data\n",
   "  • Have human coders verify LLM qualitative coding\n",
   "  • Conduct peer review of the paper draft\n",
   "  • Submit to ACM CHI (deadline typically September)\n",
   "\"\"\")"
  ],
  "outputs": [],
  "execution_count": null
 }
]