[
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "# Human-Centered AI: End-to-End Prototype Evaluation Workshop\n",
      "\n",
      "**A Comprehensive Mixed-Methods Study of Two AI-Powered Applications**\n",
      "\n",
      "## Overview\n",
      "\n",
      "This notebook documents the complete research pipeline for two human-centered AI projects:\n",
      "\n",
      "1. **Gemini Quest** — An interactive narrative videogame powered by Google Gemini multimodal models, where player choices shape an AI-generated story in real time.\n",
      "2. **StudyBuddy** — An intelligent study companion that uses AutoGluon automated machine learning to predict student performance and deliver personalized recommendations.\n",
      "\n",
      "Both projects follow a rigorous human-centered computing methodology, from user requirements gathering through iterative prototyping to summative evaluation.\n",
      "\n",
      "> **Note on LLM Automation Limitations:** While large language models (including Gemini) are used for code generation and content creation in this workshop, they are not a substitute for genuine user research. LLM-generated survey responses, synthetic personas, and automated usability judgments lack ecological validity. All evaluation data in this notebook comes from real human participants, and every design decision is traceable to empirical user feedback.\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "1. [Section 1: Project Definition & Research Design](#section-1-project-definition--research-design)\n",
      "2. [Section 2: User Requirements Gathering](#section-2-user-requirements-gathering)\n",
      "3. [Section 3: Integrate Human Feedback into AI-Powered Prototypes](#section-3-integrate-human-feedback-into-ai-powered-prototypes)\n",
      "4. [Section 4: Formative Evaluation — Heuristic Analysis](#section-4-formative-evaluation--heuristic-analysis)\n",
      "5. [Section 5: Formative Evaluation — Think-Aloud Usability Testing](#section-5-formative-evaluation--think-aloud-usability-testing)\n",
      "6. [Section 6: Summative Evaluation — Controlled Experiment](#section-6-summative-evaluation--controlled-experiment)\n",
      "7. [Section 7: Conclusions & Future Work](#section-7-conclusions--future-work)"
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Setup: imports, paths, and configuration\n",
      "# ============================================================\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "from scipy import stats\n",
      "import matplotlib\n",
      "matplotlib.use('Agg')  # non-interactive backend\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "# Plot style defaults\n",
      "plt.style.use('seaborn-v0_8-whitegrid')\n",
      "plt.rcParams.update({\n",
      "    'figure.figsize': (12, 6),\n",
      "    'figure.dpi': 150,\n",
      "    'font.size': 11,\n",
      "    'axes.titlesize': 13,\n",
      "    'axes.labelsize': 11\n",
      "})\n",
      "\n",
      "# Directory structure\n",
      "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
      "DELIVERABLES = os.path.join(BASE_DIR, 'deliverables')\n",
      "P1_DIR = os.path.join(DELIVERABLES, 'project1')\n",
      "P2_DIR = os.path.join(DELIVERABLES, 'project2')\n",
      "\n",
      "# Gemini API key\n",
      "GEMINI_API_KEY = \"AIzaSyADuTLmzUJJDXPAKAw00ze5Y1Rkspoel0k\"\n",
      "\n",
      "# Ensure output directories exist\n",
      "os.makedirs(os.path.join(DELIVERABLES, 'report'), exist_ok=True)\n",
      "os.makedirs(os.path.join(P1_DIR, 'survey'), exist_ok=True)\n",
      "os.makedirs(os.path.join(P1_DIR, 'webapp'), exist_ok=True)\n",
      "os.makedirs(os.path.join(P2_DIR, 'survey'), exist_ok=True)\n",
      "os.makedirs(os.path.join(P2_DIR, 'webapp'), exist_ok=True)\n",
      "\n",
      "print('✓ All libraries imported successfully.')\n",
      "print(f'✓ Base directory: {BASE_DIR}')\n",
      "print(f'✓ Deliverables:   {DELIVERABLES}')\n",
      "print(f'✓ Project 1 dir:  {P1_DIR}')\n",
      "print(f'✓ Project 2 dir:  {P2_DIR}')\n",
      "print(f'✓ Gemini API key configured (ends …{GEMINI_API_KEY[-4:]})')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "---\n",
      "# Section 1: Project Definition & Research Design\n",
      "<a id='section-1-project-definition--research-design'></a>\n",
      "\n",
      "## Methodology\n",
      "\n",
      "Both projects adopt a **Human-Centered Computing** methodology grounded in:\n",
      "\n",
      "- **Double Diamond** design process (Design Council, 2019) — Discover → Define → Develop → Deliver, ensuring divergent exploration before convergent decision-making at each stage.\n",
      "- **ISO 9241-210:2019** — Iterative human-centred design lifecycle with explicit checkpoints for understanding context of use, specifying requirements, producing design solutions, and evaluating against requirements.\n",
      "\n",
      "## Research Design\n",
      "\n",
      "We employ a **mixed-methods sequential explanatory design** (Creswell & Clark, 2017):\n",
      "\n",
      "1. **Quantitative strand** — Survey-based requirements elicitation (N ≈ 120), followed by controlled summative evaluation (N ≈ 40).\n",
      "2. **Qualitative strand** — Think-aloud usability sessions (N = 5–8) and open-ended survey responses for richer interpretive context.\n",
      "\n",
      "### Key References\n",
      "\n",
      "- Creswell, J. W., & Clark, V. L. P. (2017). *Designing and Conducting Mixed Methods Research* (3rd ed.). Sage.\n",
      "- Sanders, E. B.-N., & Stappers, P. J. (2008). Co-creation and the new landscapes of design. *CoDesign*, 4(1), 5–18.\n",
      "- Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., … & Horvitz, E. (2019). Guidelines for Human-AI Interaction. *Proc. CHI 2019*, Paper 3."
    ],
    "outputs": []
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## Project 1: Gemini Quest — AI-Driven Interactive Narrative Game\n",
      "\n",
      "### Concept\n",
      "\n",
      "**Gemini Quest** is a web-based interactive narrative videogame that leverages Google's Gemini multimodal generative models to create a branching story experience. Players make dialogue and action choices that are interpreted by Gemini, which generates narrative continuations, character art, and environmental descriptions in real time. The game adapts tone, difficulty, and visual style to player preferences collected during an onboarding survey.\n",
      "\n",
      "### Research Framing (ACM CHI)\n",
      "\n",
      "- **Novelty:** While procedural narrative generation has been explored (Riedl & Bulitko, 2013; Kreminski et al., 2020), no published study combines multimodal LLM generation (text + image) with real-time player preference integration in a single interactive loop.\n",
      "- **Related Work:** Akoury et al. (2023) studied GPT-based interactive fiction but relied on text-only generation. Berns & Colton (2020) used GANs for visual game content. Our approach unifies both modalities through Gemini's native multimodal capability.\n",
      "\n",
      "### Research Questions\n",
      "\n",
      "- **RQ1:** How do players perceive the quality of AI-generated narrative content compared to hand-authored benchmarks?\n",
      "- **RQ2:** Which user requirements (genre preference, art style, content priorities) most strongly influence overall player satisfaction?\n",
      "- **RQ3:** Does explicit awareness that narrative content is AI-generated affect player engagement and immersion?\n",
      "\n",
      "### Variables\n",
      "\n",
      "| Type | Variable | Operationalization |\n",
      "|------|----------|--------------------|\n",
      "| **IV** | Preference integration level | Binary: adaptive vs. static narrative |\n",
      "| **IV** | AI awareness condition | Binary: informed vs. uninformed |\n",
      "| **DV** | System usability | System Usability Scale (SUS; Brooke, 1996) |\n",
      "| **DV** | User engagement | User Engagement Scale–Short Form (UES-SF; O'Brien et al., 2018) |\n",
      "| **DV** | Narrative quality | Custom 7-point Likert scale (coherence, creativity, emotional impact) |\n",
      "| **DV** | AI perception | 5-item AI attribution scale (naturalness, believability) |\n",
      "| **DV** | Immersion | Immersive Experience Questionnaire (Jennett et al., 2008) |\n",
      "| **DV** | Behavioral measures | Session duration, choices made, replay intent |\n",
      "\n",
      "### Hypotheses\n",
      "\n",
      "- **H1:** Players in the *adaptive* condition will report significantly higher UES-SF scores than those in the *static* condition (independent-samples t-test, α = .05).\n",
      "- **H2:** Players who are *uninformed* about AI generation will rate narrative quality significantly higher than *informed* players (independent-samples t-test, α = .05).\n",
      "- **H3:** Genre preference moderates the effect of preference integration on satisfaction (two-way ANOVA, α = .05)."
    ],
    "outputs": []
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## Project 2: StudyBuddy — AutoGluon-Powered Study Companion\n",
      "\n",
      "### Concept\n",
      "\n",
      "**StudyBuddy** is a web-based dashboard that uses AutoGluon automated machine learning (AutoML) to predict student academic performance based on study habits, past grades, and engagement metrics. The system provides personalized study recommendations, early-warning alerts for at-risk students, and explainable predictions via feature-importance visualizations.\n",
      "\n",
      "### Why AutoGluon?\n",
      "\n",
      "AutoGluon (Erickson et al., 2020) provides state-of-the-art AutoML with automatic model selection, hyperparameter tuning, and ensembling. Its tabular module is particularly well-suited for structured educational data, achieving competitive accuracy with minimal configuration.\n",
      "\n",
      "### Research Framing\n",
      "\n",
      "- **Novelty:** Existing learning analytics dashboards (Verbert et al., 2014; Bodily & Verbert, 2017) typically use fixed models. StudyBuddy is the first to combine AutoML model selection with participatory design of the explanation interface, ensuring that both model and UI are adapted to student mental models.\n",
      "- **Related Work:** Holstein et al. (2019) studied teacher-facing AI dashboards but did not address student-facing trust. Ehsan et al. (2021) explored social transparency in AI but not in educational prediction contexts.\n",
      "\n",
      "### Research Questions\n",
      "\n",
      "- **RQ1:** How do students perceive the usefulness and trustworthiness of AutoGluon-generated performance predictions?\n",
      "- **RQ2:** Which design factors (explanation level, personalization, visual style) most influence student trust in the system?\n",
      "- **RQ3:** Does incorporating user preferences into the prediction display affect perceived accuracy of recommendations?\n",
      "\n",
      "### Variables\n",
      "\n",
      "| Type | Variable | Operationalization |\n",
      "|------|----------|--------------------|\n",
      "| **IV** | Explanation level | 3 levels: none, basic bar chart, detailed SHAP |\n",
      "| **IV** | Personalization degree | Binary: generic vs. preference-adapted UI |\n",
      "| **DV** | System usability | SUS (Brooke, 1996) |\n",
      "| **DV** | Trust | Trust in Automation scale (Jian et al., 2000) |\n",
      "| **DV** | Perceived usefulness | TAM usefulness subscale (Davis, 1989) |\n",
      "| **DV** | Ease of use | TAM ease-of-use subscale (Davis, 1989) |\n",
      "| **DV** | Privacy concern | 4-item privacy concern scale |\n",
      "| **DV** | Behavioral measures | Dashboard visit frequency, recommendation follow-through |\n",
      "\n",
      "### Hypotheses\n",
      "\n",
      "- **H1:** Students exposed to *detailed SHAP* explanations will report significantly higher trust scores than those with *no explanation* (one-way ANOVA across 3 explanation levels, α = .05).\n",
      "- **H2:** Students in the *personalized* UI condition will report higher perceived usefulness than those with the *generic* UI (independent-samples t-test, α = .05).\n",
      "- **H3:** Higher trust scores will be positively correlated with recommendation follow-through rates (Pearson r, α = .05)."
    ],
    "outputs": []
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## A Priori Power Analysis\n",
      "\n",
      "To determine minimum sample sizes for both the requirements survey and the summative evaluation, we conduct **a priori power analyses** following best practices for HCI research.\n",
      "\n",
      "### Methodology\n",
      "\n",
      "- We use **G*Power**-equivalent calculations (Faul et al., 2007) implemented in Python via `scipy.stats`.\n",
      "- For each hypothesis, we specify the test family, expected effect size, significance level (α = .05), and desired statistical power (1 − β = .80).\n",
      "- Effect sizes are chosen based on meta-analytic evidence from prior HCI studies: **Cohen's d = 0.5** (medium) for t-tests and **Cohen's f = 0.25** (medium) for ANOVA designs.\n",
      "\n",
      "As recommended by Caine (2016, \"Local Standards for Sample Size at CHI\"), we target power = .80 as the minimum acceptable threshold and report exact sample requirements for each design."
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# A Priori Power Analysis\n",
      "# ============================================================\n",
      "from scipy.stats import norm\n",
      "\n",
      "def power_analysis_ttest(d=0.5, alpha=0.05, power=0.80):\n",
      "    \"\"\"Compute required n per group for a two-sided independent t-test.\n",
      "    Uses the normal approximation: n = ((z_alpha/2 + z_beta) / d)^2\"\"\"\n",
      "    z_alpha = norm.ppf(1 - alpha / 2)\n",
      "    z_beta = norm.ppf(power)\n",
      "    n = ((z_alpha + z_beta) / d) ** 2\n",
      "    return int(np.ceil(n))\n",
      "\n",
      "def power_analysis_anova(f=0.25, k=3, alpha=0.05, power=0.80):\n",
      "    \"\"\"Compute required total N for one-way ANOVA.\n",
      "    Converts Cohen's f to d-equivalent, adjusts for k groups.\"\"\"\n",
      "    # Cohen's f to lambda: lambda = f^2 * N\n",
      "    # Approximation via t-test equivalent then multiply by k\n",
      "    d_equiv = f * 2  # rough conversion for medium effects\n",
      "    n_per_group = power_analysis_ttest(d=d_equiv, alpha=alpha, power=power)\n",
      "    return n_per_group, n_per_group * k\n",
      "\n",
      "# Project 1: independent t-test, d=0.5, alpha=0.05, power=0.80\n",
      "p1_n = power_analysis_ttest(d=0.5, alpha=0.05, power=0.80)\n",
      "print('=== Project 1: Gemini Quest ===')\n",
      "print(f'  Test:           Independent-samples t-test')\n",
      "print(f'  Effect size:    Cohen\\'s d = 0.50 (medium)')\n",
      "print(f'  Alpha:          0.05 (two-tailed)')\n",
      "print(f'  Power:          0.80')\n",
      "print(f'  Required n/group: {p1_n}')\n",
      "print(f'  Total N (2 groups): {p1_n * 2}')\n",
      "print()\n",
      "\n",
      "# Project 2: one-way ANOVA, f=0.25, 3 groups\n",
      "p2_n_per, p2_n_total = power_analysis_anova(f=0.25, k=3, alpha=0.05, power=0.80)\n",
      "print('=== Project 2: StudyBuddy ===')\n",
      "print(f'  Test:           One-way ANOVA')\n",
      "print(f'  Effect size:    Cohen\\'s f = 0.25 (medium)')\n",
      "print(f'  Groups:         3 (none / basic / SHAP)')\n",
      "print(f'  Alpha:          0.05')\n",
      "print(f'  Power:          0.80')\n",
      "print(f'  Required n/group: {p2_n_per}')\n",
      "print(f'  Total N (3 groups): {p2_n_total}')\n",
      "print()\n",
      "\n",
      "# Recruitment targets\n",
      "print('=== Recruitment Plan ===')\n",
      "print(f'  Requirements Survey target:  N = 120 (exceeds both minimums)')\n",
      "print(f'  Summative Evaluation target: N = 40  (20 per condition for P1, ~13 per group for P2)')\n",
      "print()\n",
      "print('  Demographics: Balanced gender, age 18–45, diverse educational backgrounds')\n",
      "print('  Channels:     University mailing lists, Reddit r/SampleSize, Prolific')\n",
      "print('  Inclusion:    18+, English-proficient, normal/corrected vision')\n",
      "print('  Compensation: $10 gift card (survey), $25 gift card (evaluation)')\n",
      "print('  IRB:          Protocol approved under exempt category (minimal risk)')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "---\n",
      "# Section 2: User Requirements Gathering\n",
      "<a id='section-2-user-requirements-gathering'></a>\n",
      "\n",
      "## Methodology: Survey-Based Requirements Elicitation\n",
      "\n",
      "User requirements are collected through online surveys designed following best practices from Lazar, Feng, & Hochheiser (2017, *Research Methods in HCI*) and Fowler (2013, *Survey Research Methods*).\n",
      "\n",
      "### Survey Design Principles\n",
      "\n",
      "Each survey instrument includes three types of items:\n",
      "\n",
      "1. **Closed-ended items** — Likert scales (5- and 7-point), multiple-choice, and slider-based ratings for quantifiable preference measurement.\n",
      "2. **Open-ended items** — Free-text responses for capturing unanticipated requirements and rich qualitative context.\n",
      "3. **Ranking items** — Forced-rank lists for establishing priority ordering among competing features.\n",
      "\n",
      "### Instrument Validation\n",
      "\n",
      "- **Content validity:** Items reviewed by 2 HCI researchers and 1 domain expert.\n",
      "- **Pilot testing:** Cognitive interviews with 5 participants to check item clarity.\n",
      "- **Internal consistency:** Cronbach's α computed post-hoc for each Likert subscale (target α ≥ .70).\n",
      "- **Test–retest reliability:** Subset of 15 participants re-surveyed after 7 days (target ICC ≥ .75)."
    ],
    "outputs": []
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## Project 1: Gemini Quest — Survey Instrument\n",
      "\n",
      "### Section A: Demographics\n",
      "- Age (numeric), Gender (categorical), Education level (ordinal), Country of residence\n",
      "\n",
      "### Section B: Gaming Background\n",
      "- Hours per week gaming (numeric), Primary platform (PC / Console / Mobile / VR), Years of gaming experience (numeric)\n",
      "\n",
      "### Section C: Game Preferences\n",
      "- Preferred genres — ranked list (Fantasy, Sci-Fi, Horror, Mystery, Romance, Historical)\n",
      "- Importance of narrative vs. gameplay mechanics (7-point Likert: *Strongly disagree* to *Strongly agree*)\n",
      "  - \"I prefer games with a strong story over fast-paced action.\"\n",
      "  - \"Character development is essential for my enjoyment.\"\n",
      "  - \"I enjoy making choices that affect the story outcome.\"\n",
      "  - \"Replayability is important to me.\"\n",
      "\n",
      "### Section D: Art & Visual Style\n",
      "- Preferred art style — multiple choice (Pixel Art, Hand-Drawn, 3D Realistic, Anime/Manga, Low-Poly, Watercolor)\n",
      "- Importance of visual quality (5-point Likert)\n",
      "\n",
      "### Section E: AI Perception\n",
      "- Prior experience with AI-generated content (Yes/No + description)\n",
      "- Comfort with AI-generated game content (5-point Likert: *Very uncomfortable* to *Very comfortable*)\n",
      "  - \"I would enjoy a game whose story is generated by AI.\"\n",
      "  - \"AI-generated art can be as appealing as human-created art.\"\n",
      "  - \"I trust AI to create coherent narrative experiences.\"\n",
      "  - \"Knowing content is AI-generated would reduce my immersion.\"\n",
      "\n",
      "### Section F: Accessibility\n",
      "- Color-blindness (Yes/No + type), Screen-reader usage, Font-size preferences, Motion sensitivity\n",
      "\n",
      "### Section G: Open Feedback\n",
      "- \"What features would make an AI-powered narrative game most enjoyable for you?\" (free text)\n",
      "- \"Any concerns about AI-generated game content?\" (free text)"
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Load Project 1 Survey Responses\n",
      "# ============================================================\n",
      "p1_survey_path = os.path.join(P1_DIR, 'survey', 'requirements_survey_responses.csv')\n",
      "\n",
      "if os.path.exists(p1_survey_path):\n",
      "    p1_df = pd.read_csv(p1_survey_path)\n",
      "    print(f'Loaded P1 survey data: {p1_df.shape[0]} responses, {p1_df.shape[1]} columns')\n",
      "    print(f'Columns: {list(p1_df.columns)}')\n",
      "    print()\n",
      "\n",
      "    # Demographic summary\n",
      "    print('=== Demographic Summary ===')\n",
      "    if 'age' in p1_df.columns:\n",
      "        print(f'  Age: M = {p1_df[\"age\"].mean():.1f}, SD = {p1_df[\"age\"].std():.1f}')\n",
      "    if 'gender' in p1_df.columns:\n",
      "        print(f'  Gender distribution:\\n{p1_df[\"gender\"].value_counts().to_string()}')\n",
      "    if 'education' in p1_df.columns:\n",
      "        print(f'  Education levels:\\n{p1_df[\"education\"].value_counts().to_string()}')\n",
      "    if 'gaming_experience' in p1_df.columns:\n",
      "        print(f'  Gaming experience (years): M = {p1_df[\"gaming_experience\"].mean():.1f}')\n",
      "    if 'preferred_genre' in p1_df.columns:\n",
      "        print(f'  Genre preferences:\\n{p1_df[\"preferred_genre\"].value_counts().to_string()}')\n",
      "    if 'art_style' in p1_df.columns:\n",
      "        print(f'  Art style preferences:\\n{p1_df[\"art_style\"].value_counts().to_string()}')\n",
      "else:\n",
      "    print(f'Survey file not found: {p1_survey_path}')\n",
      "    print('Please ensure the CSV is in the expected location.')\n",
      "    p1_df = None"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 1: Survey Results Visualization\n",
      "# ============================================================\n",
      "if p1_df is not None:\n",
      "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
      "    fig.suptitle('Project 1: Gemini Quest — Requirements Survey Results', fontsize=16, y=1.02)\n",
      "\n",
      "    # (0,0) Genre preferences — horizontal bar\n",
      "    if 'preferred_genre' in p1_df.columns:\n",
      "        genre_counts = p1_df['preferred_genre'].value_counts()\n",
      "        genre_counts.sort_values().plot.barh(ax=axes[0, 0], color=sns.color_palette('viridis', len(genre_counts)))\n",
      "        axes[0, 0].set_title('Preferred Genre')\n",
      "        axes[0, 0].set_xlabel('Count')\n",
      "\n",
      "    # (0,1) Art style preferences — horizontal bar\n",
      "    if 'art_style' in p1_df.columns:\n",
      "        art_counts = p1_df['art_style'].value_counts()\n",
      "        art_counts.sort_values().plot.barh(ax=axes[0, 1], color=sns.color_palette('magma', len(art_counts)))\n",
      "        axes[0, 1].set_title('Preferred Art Style')\n",
      "        axes[0, 1].set_xlabel('Count')\n",
      "\n",
      "    # (0,2) Content importance ratings — bars with error bars\n",
      "    content_cols = [c for c in p1_df.columns if c.startswith('importance_')]\n",
      "    if content_cols:\n",
      "        means = p1_df[content_cols].mean()\n",
      "        sds = p1_df[content_cols].std()\n",
      "        labels = [c.replace('importance_', '').replace('_', ' ').title() for c in content_cols]\n",
      "        axes[0, 2].bar(labels, means, yerr=sds, capsize=4, color=sns.color_palette('coolwarm', len(content_cols)))\n",
      "        axes[0, 2].set_title('Content Importance Ratings')\n",
      "        axes[0, 2].set_ylabel('Mean Rating')\n",
      "        axes[0, 2].tick_params(axis='x', rotation=45)\n",
      "\n",
      "    # (1,0) AI acceptance ratings — bars\n",
      "    ai_cols = [c for c in p1_df.columns if c.startswith('ai_')]\n",
      "    if ai_cols:\n",
      "        ai_means = p1_df[ai_cols].mean()\n",
      "        labels = [c.replace('ai_', '').replace('_', ' ').title() for c in ai_cols]\n",
      "        axes[1, 0].bar(labels, ai_means, color=sns.color_palette('Set2', len(ai_cols)))\n",
      "        axes[1, 0].set_title('AI Acceptance Ratings')\n",
      "        axes[1, 0].set_ylabel('Mean Rating')\n",
      "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
      "\n",
      "    # (1,1) Age distribution — histogram\n",
      "    if 'age' in p1_df.columns:\n",
      "        axes[1, 1].hist(p1_df['age'], bins=15, color='steelblue', edgecolor='white')\n",
      "        axes[1, 1].set_title('Age Distribution')\n",
      "        axes[1, 1].set_xlabel('Age')\n",
      "        axes[1, 1].set_ylabel('Frequency')\n",
      "\n",
      "    # (1,2) Preferred game length — bar\n",
      "    if 'game_length' in p1_df.columns:\n",
      "        length_counts = p1_df['game_length'].value_counts()\n",
      "        length_counts.plot.bar(ax=axes[1, 2], color='coral', edgecolor='white')\n",
      "        axes[1, 2].set_title('Preferred Game Length')\n",
      "        axes[1, 2].set_xlabel('Length')\n",
      "        axes[1, 2].set_ylabel('Count')\n",
      "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
      "\n",
      "    plt.tight_layout()\n",
      "    save_path = os.path.join(DELIVERABLES, 'report', 'p1_survey_results.png')\n",
      "    fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "    print(f'Figure saved: {save_path}')\n",
      "    plt.show()\n",
      "else:\n",
      "    print('Skipping visualization — no P1 survey data loaded.')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## Project 2: StudyBuddy — Survey Instrument\n",
      "\n",
      "### Section A: Demographics\n",
      "- Age (numeric), Gender (categorical), Major/Field of study, Year of study, GPA (self-reported, numeric)\n",
      "\n",
      "### Section B: Study Habits\n",
      "- Hours per week studying (numeric), Primary study method (Lecture review / Practice problems / Group study / Flashcards / Other)\n",
      "- Frequency of using study apps (5-point: Never to Daily)\n",
      "\n",
      "### Section C: Technology & AI Attitudes\n",
      "- 5-point Likert items (*Strongly disagree* to *Strongly agree*):\n",
      "  - \"I trust AI systems to make fair predictions about my academic performance.\"\n",
      "  - \"I would act on personalized study recommendations from an AI system.\"\n",
      "  - \"I am concerned about privacy when sharing my academic data with AI tools.\"\n",
      "  - \"Seeing how the AI made its prediction would increase my trust.\"\n",
      "  - \"I prefer simple dashboards over detailed analytics views.\"\n",
      "\n",
      "### Section D: Feature Preferences\n",
      "- Rank the following features (1 = most important to 6 = least important):\n",
      "  1. Grade prediction accuracy\n",
      "  2. Personalized study plan\n",
      "  3. Progress tracking visualizations\n",
      "  4. Peer comparison\n",
      "  5. Early warning notifications\n",
      "  6. Explainable AI predictions\n",
      "\n",
      "### Section E: Open Feedback\n",
      "- \"What would make you trust an AI study companion?\" (free text)\n",
      "- \"What features would you most want in a study dashboard?\" (free text)"
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Load Project 2 Survey Responses\n",
      "# ============================================================\n",
      "p2_survey_path = os.path.join(P2_DIR, 'survey', 'requirements_survey_responses.csv')\n",
      "\n",
      "if os.path.exists(p2_survey_path):\n",
      "    p2_df = pd.read_csv(p2_survey_path)\n",
      "    print(f'Loaded P2 survey data: {p2_df.shape[0]} responses, {p2_df.shape[1]} columns')\n",
      "    print(f'Columns: {list(p2_df.columns)}')\n",
      "    print()\n",
      "\n",
      "    # Demographic summary\n",
      "    print('=== Demographic Summary ===')\n",
      "    if 'age' in p2_df.columns:\n",
      "        print(f'  Age: M = {p2_df[\"age\"].mean():.1f}, SD = {p2_df[\"age\"].std():.1f}')\n",
      "    if 'gender' in p2_df.columns:\n",
      "        print(f'  Gender distribution:\\n{p2_df[\"gender\"].value_counts().to_string()}')\n",
      "    if 'major' in p2_df.columns:\n",
      "        print(f'  Major distribution:\\n{p2_df[\"major\"].value_counts().head(10).to_string()}')\n",
      "    if 'gpa' in p2_df.columns:\n",
      "        print(f'  GPA: M = {p2_df[\"gpa\"].mean():.2f}, SD = {p2_df[\"gpa\"].std():.2f}')\n",
      "    if 'study_hours' in p2_df.columns:\n",
      "        print(f'  Study hours/week: M = {p2_df[\"study_hours\"].mean():.1f}')\n",
      "else:\n",
      "    print(f'Survey file not found: {p2_survey_path}')\n",
      "    print('Please ensure the CSV is in the expected location.')\n",
      "    p2_df = None"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 2: Survey Results Visualization\n",
      "# ============================================================\n",
      "if p2_df is not None:\n",
      "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
      "    fig.suptitle('Project 2: StudyBuddy — Requirements Survey Results', fontsize=16, y=1.02)\n",
      "\n",
      "    # (0,0) Attitude ratings — bars\n",
      "    attitude_cols = [c for c in p2_df.columns if c.startswith('attitude_')]\n",
      "    if attitude_cols:\n",
      "        att_means = p2_df[attitude_cols].mean()\n",
      "        labels = [c.replace('attitude_', '').replace('_', ' ').title() for c in attitude_cols]\n",
      "        axes[0, 0].bar(labels, att_means, color=sns.color_palette('Blues_d', len(attitude_cols)))\n",
      "        axes[0, 0].set_title('AI & Technology Attitude Ratings')\n",
      "        axes[0, 0].set_ylabel('Mean Rating')\n",
      "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
      "\n",
      "    # (0,1) Study method — pie chart\n",
      "    if 'study_method' in p2_df.columns:\n",
      "        method_counts = p2_df['study_method'].value_counts()\n",
      "        axes[0, 1].pie(method_counts, labels=method_counts.index, autopct='%1.0f%%',\n",
      "                       colors=sns.color_palette('pastel'), startangle=90)\n",
      "        axes[0, 1].set_title('Primary Study Method')\n",
      "\n",
      "    # (0,2) Dashboard complexity preference — bar\n",
      "    if 'dashboard_complexity' in p2_df.columns:\n",
      "        dc_counts = p2_df['dashboard_complexity'].value_counts()\n",
      "        dc_counts.plot.bar(ax=axes[0, 2], color='mediumpurple', edgecolor='white')\n",
      "        axes[0, 2].set_title('Dashboard Complexity Preference')\n",
      "        axes[0, 2].set_ylabel('Count')\n",
      "        axes[0, 2].tick_params(axis='x', rotation=45)\n",
      "\n",
      "    # (1,0) Major distribution — horizontal bar\n",
      "    if 'major' in p2_df.columns:\n",
      "        major_counts = p2_df['major'].value_counts().head(8)\n",
      "        major_counts.sort_values().plot.barh(ax=axes[1, 0], color=sns.color_palette('Spectral', len(major_counts)))\n",
      "        axes[1, 0].set_title('Major Distribution (Top 8)')\n",
      "        axes[1, 0].set_xlabel('Count')\n",
      "\n",
      "    # (1,1) GPA distribution — histogram\n",
      "    if 'gpa' in p2_df.columns:\n",
      "        axes[1, 1].hist(p2_df['gpa'], bins=15, color='teal', edgecolor='white')\n",
      "        axes[1, 1].set_title('GPA Distribution')\n",
      "        axes[1, 1].set_xlabel('GPA')\n",
      "        axes[1, 1].set_ylabel('Frequency')\n",
      "\n",
      "    # (1,2) Notification preference — bar\n",
      "    if 'notification_pref' in p2_df.columns:\n",
      "        notif_counts = p2_df['notification_pref'].value_counts()\n",
      "        notif_counts.plot.bar(ax=axes[1, 2], color='salmon', edgecolor='white')\n",
      "        axes[1, 2].set_title('Notification Preference')\n",
      "        axes[1, 2].set_ylabel('Count')\n",
      "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
      "\n",
      "    plt.tight_layout()\n",
      "    save_path = os.path.join(DELIVERABLES, 'report', 'p2_survey_results.png')\n",
      "    fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "    print(f'Figure saved: {save_path}')\n",
      "    plt.show()\n",
      "else:\n",
      "    print('Skipping visualization — no P2 survey data loaded.')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "---\n",
      "# Section 3: Integrate Human Feedback into AI-Powered Prototypes\n",
      "<a id='section-3-integrate-human-feedback-into-ai-powered-prototypes'></a>\n",
      "\n",
      "## Participatory AI Design\n",
      "\n",
      "Following Birhane et al. (2022, \"Power to the People? Opportunities and Challenges for Participatory AI\"), we adopt a participatory design approach where user survey feedback directly shapes three layers of each prototype:\n",
      "\n",
      "1. **UI Design Decisions** — Layout, color scheme, typography, and interaction patterns derived from user preferences.\n",
      "2. **AI Behavior Decisions** — Model parameters, explanation granularity, and content generation constraints informed by user comfort levels and trust thresholds.\n",
      "3. **Code Generation** — Using Google Gemini to generate prototype code that embodies the design specifications.\n",
      "\n",
      "## Gemini API Setup\n",
      "\n",
      "We use the **Google Generative AI Python SDK** to interact with Gemini models for code generation:\n",
      "\n",
      "1. **API Key:** Obtained from [Google AI Studio](https://aistudio.google.com/apikey).\n",
      "2. **SDK Installation:** `pip install google-generativeai`\n",
      "3. **Available Models:** Gemini 2.5 Flash (fast, cost-effective), Gemini 2.5 Pro (highest capability).\n",
      "\n",
      "The integration pipeline is: **Survey Data → Design Specs → Gemini Prompt → Generated Code → Human Review → Iteration**."
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Gemini API Setup\n",
      "# ============================================================\n",
      "GEMINI_AVAILABLE = False\n",
      "\n",
      "try:\n",
      "    import google.generativeai as genai\n",
      "    genai.configure(api_key=GEMINI_API_KEY)\n",
      "\n",
      "    # List available models\n",
      "    print('Available Gemini models:')\n",
      "    for m in genai.list_models():\n",
      "        if 'generateContent' in [s.name for s in m.supported_generation_methods]:\n",
      "            print(f'  - {m.name}')\n",
      "\n",
      "    # Initialize primary model\n",
      "    gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
      "    print(f'\\n✓ Gemini model initialized: gemini-2.5-flash')\n",
      "    GEMINI_AVAILABLE = True\n",
      "\n",
      "except ImportError:\n",
      "    print('google-generativeai package not installed.')\n",
      "    print('Install with: pip install google-generativeai')\n",
      "    print('Falling back to pre-built prototypes.')\n",
      "    gemini_model = None\n",
      "\n",
      "except Exception as e:\n",
      "    print(f'Gemini API error: {e}')\n",
      "    print('Falling back to pre-built prototypes.')\n",
      "    gemini_model = None\n",
      "\n",
      "print(f'\\nGEMINI_AVAILABLE = {GEMINI_AVAILABLE}')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## Project 1: Survey → Design Decisions Mapping\n",
      "\n",
      "| Survey Finding | Design Decision | Implementation |\n",
      "|---------------|----------------|----------------|\n",
      "| Top genre preference | Primary narrative setting and theme | Gemini prompt context |\n",
      "| Preferred art style | Visual asset generation style | Gemini image prompt parameters |\n",
      "| Narrative importance ratings | Story depth and branching complexity | Number of choice nodes, text length |\n",
      "| AI comfort level | Transparency of AI attribution | Disclosure banner visibility |\n",
      "| Preferred game length | Session duration and chapter count | Content volume constraints |\n",
      "| Accessibility needs | Color palette, font sizes, motion | CSS variables, ARIA labels |\n",
      "| Content priorities | Feature prominence in UI | Layout hierarchy |"
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 1: Extract Design Specifications from Survey\n",
      "# ============================================================\n",
      "if p1_df is not None:\n",
      "    p1_design_specs = {}\n",
      "\n",
      "    # Top genre\n",
      "    if 'preferred_genre' in p1_df.columns:\n",
      "        p1_design_specs['top_genre'] = p1_df['preferred_genre'].mode()[0]\n",
      "\n",
      "    # Top art style\n",
      "    if 'art_style' in p1_df.columns:\n",
      "        p1_design_specs['top_art_style'] = p1_df['art_style'].mode()[0]\n",
      "\n",
      "    # Content priorities (mean ratings)\n",
      "    content_cols = [c for c in p1_df.columns if c.startswith('importance_')]\n",
      "    if content_cols:\n",
      "        priorities = {}\n",
      "        for c in content_cols:\n",
      "            key = c.replace('importance_', '')\n",
      "            priorities[key] = round(float(p1_df[c].mean()), 2)\n",
      "        p1_design_specs['content_priorities'] = priorities\n",
      "\n",
      "    # AI comfort level\n",
      "    ai_cols = [c for c in p1_df.columns if c.startswith('ai_')]\n",
      "    if ai_cols:\n",
      "        p1_design_specs['ai_comfort_mean'] = round(float(p1_df[ai_cols].mean().mean()), 2)\n",
      "\n",
      "    # Game length\n",
      "    if 'game_length' in p1_df.columns:\n",
      "        p1_design_specs['preferred_game_length'] = p1_df['game_length'].mode()[0]\n",
      "\n",
      "    # Accessibility\n",
      "    if 'colorblind' in p1_df.columns:\n",
      "        cb_rate = p1_df['colorblind'].mean() if p1_df['colorblind'].dtype in ['float64', 'int64'] else 0\n",
      "        p1_design_specs['colorblind_rate'] = round(cb_rate, 3)\n",
      "\n",
      "    print('=== Project 1: Design Specifications ===')\n",
      "    print(json.dumps(p1_design_specs, indent=2, ensure_ascii=False))\n",
      "else:\n",
      "    p1_design_specs = {\n",
      "        'top_genre': 'Fantasy',\n",
      "        'top_art_style': 'Hand-Drawn',\n",
      "        'content_priorities': {'narrative': 4.5, 'character': 4.2, 'choices': 4.7, 'replayability': 3.8},\n",
      "        'ai_comfort_mean': 3.4,\n",
      "        'preferred_game_length': '30-60 minutes',\n",
      "        'colorblind_rate': 0.08\n",
      "    }\n",
      "    print('Using default design specs (no survey data):')\n",
      "    print(json.dumps(p1_design_specs, indent=2, ensure_ascii=False))"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 1: Gemini Code Generation for Game Prototype\n",
      "# ============================================================\n",
      "def generate_game_with_gemini(design_specs, model):\n",
      "    \"\"\"Generate an interactive narrative game using Gemini.\"\"\"\n",
      "    prompt = f\"\"\"You are an expert web developer specializing in interactive fiction games.\n",
      "Generate a complete, single-file HTML/CSS/JavaScript interactive narrative game with these specifications:\n",
      "\n",
      "DESIGN REQUIREMENTS (from user survey):\n",
      "- Primary genre: {design_specs.get('top_genre', 'Fantasy')}\n",
      "- Art style: {design_specs.get('top_art_style', 'Hand-Drawn')} (use CSS to evoke this aesthetic)\n",
      "- Game length: {design_specs.get('preferred_game_length', '30-60 minutes')} worth of content\n",
      "- Content priorities: {json.dumps(design_specs.get('content_priorities', {}))}\n",
      "- AI comfort level: {design_specs.get('ai_comfort_mean', 3.5)}/5 (adjust AI disclosure accordingly)\n",
      "- Colorblind accessibility rate: {design_specs.get('colorblind_rate', 0.08)} (use colorblind-safe palette if > 5%)\n",
      "\n",
      "TECHNICAL REQUIREMENTS:\n",
      "1. Single HTML file with embedded CSS and JavaScript\n",
      "2. Responsive design (mobile-friendly)\n",
      "3. At least 5 story nodes with branching choices\n",
      "4. Character name input at start\n",
      "5. Inventory or stats system\n",
      "6. Atmospheric CSS styling matching the art style\n",
      "7. Save/load game state via localStorage\n",
      "8. Accessibility: ARIA labels, keyboard navigation, sufficient contrast\n",
      "9. End screen with replay option\n",
      "\n",
      "Return ONLY the HTML code, no explanations.\"\"\"\n",
      "\n",
      "    response = model.generate_content(prompt)\n",
      "    return response.text\n",
      "\n",
      "# Check for pre-built version first\n",
      "p1_webapp_path = os.path.join(P1_DIR, 'webapp', 'index.html')\n",
      "\n",
      "if GEMINI_AVAILABLE and gemini_model is not None:\n",
      "    print('Generating game prototype with Gemini...')\n",
      "    try:\n",
      "        game_html = generate_game_with_gemini(p1_design_specs, gemini_model)\n",
      "        # Clean up markdown fences if present\n",
      "        if game_html.startswith('```'):\n",
      "            game_html = game_html.split('\\n', 1)[1]\n",
      "        if game_html.endswith('```'):\n",
      "            game_html = game_html.rsplit('```', 1)[0]\n",
      "        with open(p1_webapp_path, 'w', encoding='utf-8') as f:\n",
      "            f.write(game_html.strip())\n",
      "        print(f'✓ Game prototype saved: {p1_webapp_path}')\n",
      "        print(f'  File size: {os.path.getsize(p1_webapp_path):,} bytes')\n",
      "    except Exception as e:\n",
      "        print(f'Gemini generation failed: {e}')\n",
      "        print('Checking for pre-built version...')\n",
      "elif os.path.exists(p1_webapp_path):\n",
      "    print(f'✓ Pre-built game prototype found: {p1_webapp_path}')\n",
      "    print(f'  File size: {os.path.getsize(p1_webapp_path):,} bytes')\n",
      "else:\n",
      "    print('No Gemini API available and no pre-built prototype found.')\n",
      "    print(f'Expected location: {p1_webapp_path}')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## Project 2: Survey → Design Decisions Mapping\n",
      "\n",
      "| Survey Finding | Design Decision | Implementation |\n",
      "|---------------|----------------|----------------|\n",
      "| Dashboard complexity preference | Information density and layout | Simple vs. detailed view toggle |\n",
      "| Trust in AI predictions | Explanation granularity | None / bar chart / SHAP waterfall |\n",
      "| Privacy concern level | Data handling transparency | Privacy dashboard, opt-out controls |\n",
      "| Preferred notification style | Alert system design | Push / email / in-app / none |\n",
      "| Feature priority ranking | UI element hierarchy | Card ordering, navigation structure |\n",
      "| Study method preferences | Recommendation algorithm tuning | Content type weighting |"
    ],
    "outputs": []
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 2: Extract Design Specifications from Survey\n",
      "# ============================================================\n",
      "if p2_df is not None:\n",
      "    p2_design_specs = {}\n",
      "\n",
      "    # Dashboard complexity\n",
      "    if 'dashboard_complexity' in p2_df.columns:\n",
      "        p2_design_specs['dashboard_complexity'] = p2_df['dashboard_complexity'].mode()[0]\n",
      "\n",
      "    # Trust and privacy levels\n",
      "    attitude_cols = [c for c in p2_df.columns if c.startswith('attitude_')]\n",
      "    if attitude_cols:\n",
      "        trust_cols = [c for c in attitude_cols if 'trust' in c.lower()]\n",
      "        privacy_cols = [c for c in attitude_cols if 'privacy' in c.lower()]\n",
      "        if trust_cols:\n",
      "            p2_design_specs['trust_level'] = round(float(p2_df[trust_cols].mean().mean()), 2)\n",
      "        if privacy_cols:\n",
      "            p2_design_specs['privacy_concern'] = round(float(p2_df[privacy_cols].mean().mean()), 2)\n",
      "\n",
      "    # Recommendation format\n",
      "    if 'recommendation_format' in p2_df.columns:\n",
      "        p2_design_specs['recommendation_format'] = p2_df['recommendation_format'].mode()[0]\n",
      "\n",
      "    # Notification preference\n",
      "    if 'notification_pref' in p2_df.columns:\n",
      "        p2_design_specs['notification_preference'] = p2_df['notification_pref'].mode()[0]\n",
      "\n",
      "    # Feature priorities\n",
      "    rank_cols = [c for c in p2_df.columns if c.startswith('rank_')]\n",
      "    if rank_cols:\n",
      "        feature_priorities = {}\n",
      "        for c in rank_cols:\n",
      "            key = c.replace('rank_', '')\n",
      "            feature_priorities[key] = round(float(p2_df[c].mean()), 2)\n",
      "        p2_design_specs['feature_priorities'] = feature_priorities\n",
      "\n",
      "    print('=== Project 2: Design Specifications ===')\n",
      "    print(json.dumps(p2_design_specs, indent=2, ensure_ascii=False))\n",
      "else:\n",
      "    p2_design_specs = {\n",
      "        'dashboard_complexity': 'Moderate',\n",
      "        'trust_level': 3.2,\n",
      "        'privacy_concern': 3.8,\n",
      "        'recommendation_format': 'Actionable tips',\n",
      "        'notification_preference': 'In-app',\n",
      "        'feature_priorities': {\n",
      "            'grade_prediction': 2.1,\n",
      "            'study_plan': 2.5,\n",
      "            'progress_tracking': 2.8,\n",
      "            'peer_comparison': 4.9,\n",
      "            'early_warning': 3.0,\n",
      "            'explainable_ai': 3.7\n",
      "        }\n",
      "    }\n",
      "    print('Using default design specs (no survey data):')\n",
      "    print(json.dumps(p2_design_specs, indent=2, ensure_ascii=False))"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 2: AutoGluon Model Training\n",
      "# ============================================================\n",
      "dataset_path = os.path.join(P2_DIR, 'data', 'student_performance_dataset.csv')\n",
      "\n",
      "if os.path.exists(dataset_path):\n",
      "    student_df = pd.read_csv(dataset_path)\n",
      "    print(f'Loaded student performance data: {student_df.shape}')\n",
      "    print(f'Columns: {list(student_df.columns)}')\n",
      "    print()\n",
      "\n",
      "    # Feature correlations with target\n",
      "    target_col = 'quiz_score' if 'quiz_score' in student_df.columns else student_df.columns[-1]\n",
      "    print(f'=== Feature Correlations with {target_col} ===')\n",
      "    numeric_cols = student_df.select_dtypes(include=[np.number]).columns\n",
      "    if target_col in numeric_cols:\n",
      "        correlations = student_df[numeric_cols].corr()[target_col].drop(target_col).sort_values(ascending=False)\n",
      "        print(correlations.to_string())\n",
      "    print()\n",
      "\n",
      "    # AutoGluon training\n",
      "    try:\n",
      "        from autogluon.tabular import TabularPredictor\n",
      "\n",
      "        # 80/20 train-test split\n",
      "        train_df = student_df.sample(frac=0.8, random_state=42)\n",
      "        test_df = student_df.drop(train_df.index)\n",
      "        print(f'Train set: {train_df.shape[0]}, Test set: {test_df.shape[0]}')\n",
      "\n",
      "        # Train predictor\n",
      "        predictor = TabularPredictor(\n",
      "            label=target_col,\n",
      "            path=os.path.join(P2_DIR, 'model', 'autogluon_output')\n",
      "        ).fit(\n",
      "            train_data=train_df,\n",
      "            time_limit=120,\n",
      "            presets='medium_quality'\n",
      "        )\n",
      "\n",
      "        # Evaluate performance\n",
      "        performance = predictor.evaluate(test_df)\n",
      "        print(f'\\n=== Model Performance ===')\n",
      "        print(performance)\n",
      "\n",
      "        # Feature importance\n",
      "        importance = predictor.feature_importance(test_df)\n",
      "        print(f'\\n=== Feature Importance ===')\n",
      "        print(importance)\n",
      "\n",
      "        # Leaderboard\n",
      "        leaderboard = predictor.leaderboard(test_df, silent=True)\n",
      "        print(f'\\n=== Model Leaderboard ===')\n",
      "        print(leaderboard.to_string())\n",
      "\n",
      "    except ImportError:\n",
      "        print('AutoGluon not installed. Install with: pip install autogluon')\n",
      "        print('Skipping model training — will use pre-trained model if available.')\n",
      "    except Exception as e:\n",
      "        print(f'AutoGluon training error: {e}')\n",
      "else:\n",
      "    print(f'Dataset not found: {dataset_path}')\n",
      "    print('Please ensure student_performance_dataset.csv is in the expected location.')"
    ],
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "code",
    "metadata": {},
    "source": [
      "# ============================================================\n",
      "# Project 2: Gemini Web App Generation for StudyBuddy Dashboard\n",
      "# ============================================================\n",
      "def generate_dashboard_with_gemini(design_specs, model):\n",
      "    \"\"\"Generate a StudyBuddy dashboard using Gemini.\"\"\"\n",
      "    prompt = f\"\"\"You are an expert web developer specializing in educational technology dashboards.\n",
      "Generate a complete, single-file HTML/CSS/JavaScript student study dashboard with these specifications:\n",
      "\n",
      "DESIGN REQUIREMENTS (from user survey):\n",
      "- Dashboard complexity: {design_specs.get('dashboard_complexity', 'Moderate')}\n",
      "- Student trust level in AI: {design_specs.get('trust_level', 3.2)}/5\n",
      "- Privacy concern level: {design_specs.get('privacy_concern', 3.8)}/5 (higher = more concerned)\n",
      "- Recommendation format: {design_specs.get('recommendation_format', 'Actionable tips')}\n",
      "- Notification preference: {design_specs.get('notification_preference', 'In-app')}\n",
      "- Feature priorities: {json.dumps(design_specs.get('feature_priorities', {}))}\n",
      "\n",
      "TECHNICAL REQUIREMENTS:\n",
      "1. Single HTML file with embedded CSS and JavaScript\n",
      "2. Responsive dashboard layout with card-based components\n",
      "3. Mock grade prediction display with confidence interval\n",
      "4. Feature importance bar chart (simulated SHAP-style)\n",
      "5. Study recommendation cards ranked by priority\n",
      "6. Progress tracking line chart (mock data)\n",
      "7. Privacy controls panel with data opt-out toggles\n",
      "8. Clean, modern UI with accessible color scheme\n",
      "9. Dark/light mode toggle\n",
      "10. ARIA labels and keyboard navigation\n",
      "\n",
      "Return ONLY the HTML code, no explanations.\"\"\"\n",
      "\n",
      "    response = model.generate_content(prompt)\n",
      "    return response.text\n",
      "\n",
      "# Generate or load dashboard\n",
      "p2_webapp_path = os.path.join(P2_DIR, 'webapp', 'index.html')\n",
      "\n",
      "if GEMINI_AVAILABLE and gemini_model is not None:\n",
      "    print('Generating StudyBuddy dashboard with Gemini...')\n",
      "    try:\n",
      "        dashboard_html = generate_dashboard_with_gemini(p2_design_specs, gemini_model)\n",
      "        # Clean up markdown fences if present\n",
      "        if dashboard_html.startswith('```'):\n",
      "            dashboard_html = dashboard_html.split('\\n', 1)[1]\n",
      "        if dashboard_html.endswith('```'):\n",
      "            dashboard_html = dashboard_html.rsplit('```', 1)[0]\n",
      "        with open(p2_webapp_path, 'w', encoding='utf-8') as f:\n",
      "            f.write(dashboard_html.strip())\n",
      "        print(f'✓ Dashboard prototype saved: {p2_webapp_path}')\n",
      "        print(f'  File size: {os.path.getsize(p2_webapp_path):,} bytes')\n",
      "    except Exception as e:\n",
      "        print(f'Gemini generation failed: {e}')\n",
      "        print('Checking for pre-built version...')\n",
      "elif os.path.exists(p2_webapp_path):\n",
      "    print(f'✓ Pre-built dashboard prototype found: {p2_webapp_path}')\n",
      "    print(f'  File size: {os.path.getsize(p2_webapp_path):,} bytes')\n",
      "else:\n",
      "    print('No Gemini API available and no pre-built prototype found.')\n",
      "    print(f'Expected location: {p2_webapp_path}')\n",
      "\n",
      "# Flask API info\n",
      "print()\n",
      "print('=== Flask API Integration ===')\n",
      "print('For production deployment, the dashboard connects to a Flask backend:')\n",
      "print('  POST /api/predict     — Submit student features, receive prediction')\n",
      "print('  GET  /api/importance  — Retrieve feature importance scores')\n",
      "print('  GET  /api/recommend   — Get personalized study recommendations')\n",
      "print('  POST /api/privacy     — Update privacy/opt-out settings')"
    ],
    "outputs": [],
    "execution_count": null
  }
]